{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os,time\n",
    "\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.layers import Activation\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(data_array: np.ndarray, train_size: float, val_size: float):\n",
    "    \"\"\"Splits data into train/val/test sets and normalizes the data.\n",
    "\n",
    "    Args:\n",
    "        data_array: ndarray of shape `(num_time_steps, num_routes)`\n",
    "        train_size: A float value between 0.0 and 1.0 that represent the proportion of the dataset\n",
    "            to include in the train split.\n",
    "        val_size: A float value between 0.0 and 1.0 that represent the proportion of the dataset\n",
    "            to include in the validation split.\n",
    "\n",
    "    Returns:\n",
    "        `train_array`, `val_array`, `test_array`\n",
    "    \"\"\"\n",
    "\n",
    "    num_time_steps = data_array.shape[0]\n",
    "    num_train, num_val = (\n",
    "        int(num_time_steps * train_size),\n",
    "        int(num_time_steps * val_size),\n",
    "    )\n",
    "    train_array = data_array[:num_train]\n",
    "    mean, std = train_array.mean(axis=0), train_array.std(axis=0)\n",
    "\n",
    "    train_array = (train_array - mean) / std\n",
    "    val_array = (data_array[num_train : (num_train + num_val)] - mean) / std\n",
    "    test_array = (data_array[(num_train + num_val) :] - mean) / std\n",
    "\n",
    "    return train_array, val_array, test_array,mean, std\n",
    "\n",
    "def get_lstm_data(data,sequence_len):\n",
    "    features=[]\n",
    "    target=[]\n",
    "    data_len=len(data)\n",
    "    for i in range(0,data_len-sequence_len):\n",
    "        X=data.iloc[i:sequence_len+i][:].to_numpy()\n",
    "        y=data.iloc[sequence_len+i][0:11].to_numpy()\n",
    "        features.append(X)\n",
    "        target.append(y)\n",
    "    return np.array(features), np.array(target)\n",
    "\n",
    "\n",
    "def create_batch_data(X,y,train=True,buffer_size=1000,batch_size=128):\n",
    "    batch_data=tf.data.Dataset.from_tensor_slices((tf.constant(X),tf.constant(y)))\n",
    "    if train:\n",
    "        return batch_data.cache().shuffle(buffer_size).batch(batch_size)\n",
    "    else:\n",
    "        return batch_data.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: (18164, 12)\n",
      "validation set size: (7265, 12)\n",
      "test set size: (10900, 12)\n"
     ]
    }
   ],
   "source": [
    "total_data = pd.read_csv(\"processed_data.csv\")\n",
    "train_size, val_size = 0.5, 0.2\n",
    "\n",
    "train_array, val_array, test_array,mean, std = preprocess(total_data, train_size, val_size)\n",
    "print(f\"train set size: {train_array.shape}\")\n",
    "print(f\"validation set size: {val_array.shape}\")\n",
    "print(f\"test set size: {test_array.shape}\")\n",
    "\n",
    "train_datasets,train_labels=get_lstm_data(train_array,10)\n",
    "train_batch_dataset=create_batch_data(train_datasets,train_labels)\n",
    "val_datasets,val_labels=get_lstm_data(val_array,10)\n",
    "val_batch_dataset=create_batch_data(val_datasets,val_labels)\n",
    "test_datasets,test_labels=get_lstm_data(test_array,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "action_min,action_max=np.amin(train_array.iloc[:,-1]),np.amax(train_array.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.8784131874704372 3.6080323911297625\n"
     ]
    }
   ],
   "source": [
    "print(action_min,action_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=keras.Sequential([\n",
    "    layers.LSTM(units=256,input_shape=(10,12),return_sequences=True),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.LSTM(units=256,return_sequences=True),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.LSTM(units=128,return_sequences=True),\n",
    "    layers.LSTM(units=32),\n",
    "    layers.Dense(11)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',loss='mse')\n",
    "model.load_weights('best_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一回路压力平均值           8.617001\n",
      "一环路冷热段平均值        266.850468\n",
      "堆芯出口温度平均值        271.661830\n",
      "主蒸汽压力平均值           4.787349\n",
      "辅助给水流量平均值         32.502430\n",
      "重要厂用水系统压力        155.820908\n",
      "DeltaTsat平均值      24.623330\n",
      "安全壳压力平均值         101.078768\n",
      "SG1水位平均值           0.883465\n",
      "稳压器泄压箱压力           0.023395\n",
      "压缩空气系统压力           0.837326\n",
      "汽机旁路系统蒸汽阀平均开度     13.046149\n",
      "dtype: float64 \n",
      " 一回路压力平均值          3.644202\n",
      "一环路冷热段平均值        28.271359\n",
      "堆芯出口温度平均值        35.540318\n",
      "主蒸汽压力平均值          1.753527\n",
      "辅助给水流量平均值        21.474391\n",
      "重要厂用水系统压力        77.657992\n",
      "DeltaTsat平均值      8.932344\n",
      "安全壳压力平均值          0.044651\n",
      "SG1水位平均值          0.994643\n",
      "稳压器泄压箱压力          0.006594\n",
      "压缩空气系统压力          0.015622\n",
      "汽机旁路系统蒸汽阀平均开度    14.851950\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(mean,'\\n',std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SGTR_env():\n",
    "    def __init__(self,model,set_point,train_datasets,mean, std):\n",
    "        self.action_space=np.array([0]*1)\n",
    "        self.observation_space=np.array([0]*12)\n",
    "        self.mean=mean\n",
    "        self.std=std\n",
    "        self.response=[]\n",
    "        self.set_point=set_point\n",
    "        self.train_datasets=train_datasets\n",
    "        self.model=model\n",
    "        self.state= self.train_datasets[random.randint(0, train_datasets.shape[0])]\n",
    "        self.step_count=0#步数计数\n",
    "\n",
    "    def reset(self):\n",
    "        state= self.train_datasets[random.randint(0, train_datasets.shape[0])]\n",
    "        self.state=np.array(state)\n",
    "        return np.array(state)[-1]\n",
    "\n",
    "    def cal_origin_val(self,pos,now_val):\n",
    "        \"\"\"\n",
    "        计算未归一化的值\n",
    "        \"\"\"\n",
    "        val=now_val*self.std[pos]+self.mean[pos]\n",
    "        return val\n",
    "\n",
    "    def justice_down (self, next_state,step):\n",
    "\t    \"\"\"\n",
    "\t    判断是否达到失败条件，deltaT<10或70分钟内未能实现一二回路压力平衡（小于1MP）\n",
    "\t    \"\"\"\n",
    "\t    ori_deltaT = self.cal_origin_val (6, next_state[-1, 6])\n",
    "\t    #ori_pressure = self.cal_origin_val(0,next_state[-1, 0])\n",
    "\t    if ori_deltaT < 10:# or (step>4200 and ori_pressure<1):\n",
    "\t\t    return True\n",
    "\n",
    "\t    else:\n",
    "\t        return False\n",
    "\n",
    "    def step (self, action):\n",
    "\t    self.step_count+=1\n",
    "\t    self.state[-1, -1] = action\n",
    "\t    #model(test_input, training=False)\n",
    "\t    next_variable_state = np.array (self.model(np.array ([self.state]),training=False))\n",
    "\t    next_action = action\n",
    "\t    zip_state_action = np.append (next_variable_state, next_action).reshape (1, -1)\n",
    "\t    next_state = np.append (self.state, zip_state_action, axis=0)\n",
    "\t    next_state = np.delete (next_state, 0, axis=0)\n",
    "\t    ori_temp_last = self.cal_origin_val (1, next_state[-1, 1])\n",
    "\t    ori_temp_before_last = self.cal_origin_val (1, next_state[-2, 1])\n",
    "\t    temp_change_speed = (ori_temp_last - ori_temp_before_last) * 3600\n",
    "\t    reward = 100 - abs (temp_change_speed + 56)\n",
    "\t    done = self.justice_down (next_state,self.step_count)\n",
    "\n",
    "\t    return next_state[-1],reward,done,{}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q Network (1,2):  <keras.engine.functional.Functional object at 0x0000022776DACC48>\n",
      "Policy Network:  <keras.engine.functional.Functional object at 0x000002299FBAA348>\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow_probability as tfp\n",
    "# import tensorlayer as tl\n",
    "#\n",
    "# tfd = tfp.distributions\n",
    "# Normal = tfd.Normal\n",
    "#\n",
    "# tl.logging.set_verbosity(tl.logging.DEBUG)\n",
    "\n",
    "random.seed(2)\n",
    "np.random.seed(2)\n",
    "tf.random.set_seed(2)  # reproducible\n",
    "\n",
    "#####################  hyper parameters  ####################\n",
    "# choose env\n",
    "ENV = 'Pendulum-v0'\n",
    "action_range = 1.  # scale action, [-action_range, action_range]\n",
    "\n",
    "# RL training\n",
    "max_frames = 40000  # total number of steps for training\n",
    "test_frames = 300  # total number of steps for testing\n",
    "max_steps = 1000  # maximum number of steps for one episode\n",
    "batch_size = 64  # udpate batchsize\n",
    "explore_steps = 500  # 500 for random action sampling in the beginning of training\n",
    "update_itr = 3  # repeated updates for single step\n",
    "hidden_dim = 32  # size of hidden layers for networks\n",
    "q_lr = 3e-4  # q_net learning rate\n",
    "policy_lr = 3e-4  # policy_net learning rate\n",
    "policy_target_update_interval = 3  # delayed steps for updating the policy network and target networks\n",
    "explore_noise_scale = 1.0  # range of action noise for exploration\n",
    "eval_noise_scale = 0.5  # range of action noise for evaluation of action value\n",
    "reward_scale = 1.  # value range of reward\n",
    "replay_buffer_size = 30000  # size of replay buffer\n",
    "\n",
    "\n",
    "###############################  TD3  ####################################\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "\t'''\n",
    "    a ring buffer for storing transitions and sampling for training\n",
    "    :state: (state_dim,)\n",
    "    :action: (action_dim,)\n",
    "    :reward: (,), scalar\n",
    "    :next_state: (state_dim,)\n",
    "    :done: (,), scalar (0 and 1) or bool (True and False)\n",
    "    '''\n",
    "\n",
    "\tdef __init__(self, capacity):\n",
    "\t\tself.capacity = capacity  #buffer的最大值\n",
    "\t\tself.buffer = []  #buffer列表\n",
    "\t\tself.position = 0  #当前输入的位置，相当于指针\n",
    "\n",
    "\tdef push(self, state, action, reward, next_state):\n",
    "\t\t#如果buffer的长度小于最大值，也就是说，第一环的时候，需要先初始化一个“空间”，这个空间值为None，再给这个空间赋值。\n",
    "\t\tif len(self.buffer) < self.capacity:\n",
    "\t\t\tself.buffer.append(None)\n",
    "\t\tself.buffer[self.position] = (state, action, reward, next_state)\n",
    "\t\tself.position = int((self.position + 1) % self.capacity)  # as a ring buffer\n",
    "\n",
    "\tdef sample(self, batch_size):\n",
    "\t\tbatch = random.sample(self.buffer, batch_size)\n",
    "\t\tstate, action, reward, next_state = map(np.stack, zip(*batch))  # stack for each element\n",
    "\t\t'''\n",
    "        the * serves as unpack: sum(a,b) <=> batch=(a,b), sum(*batch) ;\n",
    "        zip: a=[1,2], b=[2,3], zip(a,b) => [(1, 2), (2, 3)] ;\n",
    "        the map serves as mapping the function on each list element: map(square, [2,3]) => [4,9] ;\n",
    "        np.stack((1,2)) => array([1, 2])\n",
    "        '''\n",
    "\t\treturn state, action, reward, next_state\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.buffer)\n",
    "\n",
    "\n",
    "#在代码中没有用到，但我们可以学习下，这里是直接修改gym环境的动作输出，把输出归一化。\n",
    "# class NormalizedActions(gym.ActionWrapper):\n",
    "# \t''' normalize the actions to be in reasonable range '''\n",
    "#\n",
    "# \tdef _action(self, action):\n",
    "# \t\tlow = self.action_space.low  #动作空间最小值\n",
    "# \t\thigh = self.action_space.high  #动作空间最大值\n",
    "#\n",
    "# \t\taction = low + (action + 1.0) * 0.5 * (high - low)\n",
    "# \t\taction = np.clip(action, low, high)\n",
    "#\n",
    "# \t\treturn action\n",
    "#\n",
    "# \tdef _reverse_action(self, action):\n",
    "# \t\tlow = self.action_space.low\n",
    "# \t\thigh = self.action_space.high\n",
    "#\n",
    "# \t\taction = 2 * (action - low) / (high - low) - 1\n",
    "# \t\taction = np.clip(action, low, high)\n",
    "#\n",
    "# \t\treturn action\n",
    "\n",
    "\n",
    "class QNetwork():\n",
    "\t''' the network for evaluate values of state-action pairs: Q(s,a) '''\n",
    "\n",
    "\tdef __init__(self, num_inputs, num_actions, hidden_dim, init_w=3e-3):\n",
    "\t\tsuper(QNetwork, self).__init__()\n",
    "\t\tself.input_dim = num_inputs + num_actions\n",
    "\t\tself.net = self.get_net()\n",
    "\n",
    "\t# w_init = tf.keras.initializers.glorot_normal(seed=None)\n",
    "\t# w_init = tf.random_uniform_initializer(-init_w, init_w)\n",
    "\t#\n",
    "\t# self.linear1 = Dense(n_units=hidden_dim, act=tf.nn.relu, W_init=w_init, in_channels=input_dim, name='q1')\n",
    "\t# self.linear2 = Dense(n_units=hidden_dim, act=tf.nn.relu, W_init=w_init, in_channels=hidden_dim, name='q2')\n",
    "\t# self.linear3 = Dense(n_units=1, W_init=w_init, in_channels=hidden_dim, name='q3')\n",
    "\n",
    "\tdef get_net(self, name='Q_model'):\n",
    "\t\tinputs = keras.layers.Input(shape=[self.input_dim], name='C_input')\n",
    "\t\tx = keras.layers.Dense(64, activation='tanh', name='C_l1')(inputs)\n",
    "\t\t# x = keras.layers.BatchNormalization()(x)\n",
    "\t\tx = keras.layers.Dense(32, activation='tanh', name='C_l2')(x)\n",
    "\t\t# x = keras.layers.BatchNormalization()(x)\n",
    "\t\tx = keras.layers.Dense(1, name='C_out')(x)\n",
    "\t\tnet = keras.Model(inputs=[inputs], outputs=[x], name='Critic' + name)\n",
    "\t\treturn net\n",
    "\n",
    "# def forward(self, input):\n",
    "#     x = self.tanh1(input)\n",
    "#     x = self.tanh2(x)\n",
    "#     x = self.tanh3(x)\n",
    "#     return x\n",
    "\n",
    "\n",
    "class PolicyNetwork():\n",
    "\t''' the network for generating non-determinstic (Gaussian distributed) action from the state input '''\n",
    "\n",
    "\tdef __init__(self, num_inputs, num_actions, hidden_dim, action_range=1., init_w=3e-3):\n",
    "\t\tsuper(PolicyNetwork, self).__init__()\n",
    "\t\tget_custom_objects().update({'swish': Activation(self.self_act)})\n",
    "\n",
    "\t\tself.num_inputs = num_inputs\n",
    "\t\tself.action_range = action_range\n",
    "\t\tself.num_actions = num_actions\n",
    "\t\tself.net = self.get_net()\n",
    "\n",
    "\t# w_init = tf.keras.initializers.glorot_normal(seed=None)\n",
    "\t#w_init = tf.random_uniform_initializer(-init_w, init_w)\n",
    "\n",
    "\t# self.tanh1 = keras.layers.Dense(hidden_dim, activation='rtanh',\n",
    "\t#                                   name='policy1')  #Dense(n_units=hidden_dim, act=tf.nn.rtanh, W_init=w_init, in_channels=num_inputs, name='policy1')\n",
    "\t# self.tanh2 = keras.layers.Dense(hidden_dim, activation='rtanh',\n",
    "\t#                                   name='policy2')  #Dense(n_units=hidden_dim, act=tf.nn.rtanh, W_init=w_init, in_channels=hidden_dim, name='policy2')\n",
    "\t# self.tanh3 = keras.layers.Dense(hidden_dim, activation='rtanh',\n",
    "\t#                                   name='policy3')  #Dense(n_units=hidden_dim, act=tf.nn.rtanh, W_init=w_init, in_channels=hidden_dim, name='policy3')\n",
    "\t#\n",
    "\t# self.output_tanh = keras.layers.Dense(num_actions,activation='tanh',\n",
    "\t#                                         name='policy_output')  #Dense(n_units=num_actions, W_init=w_init, b_init=tf.random_uniform_initializer(-init_w, init_w), in_channels=hidden_dim, name='policy_output')\n",
    "\n",
    "\tdef get_net(self, name='policy_model'):\n",
    "\t\t\"\"\"\n",
    "        Build actor network\n",
    "        :param input_state_shape: state\n",
    "        :param name: name\n",
    "        :return: act\n",
    "        \"\"\"\n",
    "\t\tinputs = keras.layers.Input(shape=[self.num_inputs], name='A_input')\n",
    "\t\t# x = keras.layers.BatchNormalization()(inputs)\n",
    "\t\tx = keras.layers.Dense(64, activation='tanh', name='policy1')(inputs)\n",
    "\t\t# x = keras.layers.BatchNormalization()(x)\n",
    "\t\tx = keras.layers.Dense(32, activation='tanh', name='policy2')(x)\n",
    "\t\t# x = keras.layers.BatchNormalization()(x)\n",
    "\t\tx = keras.layers.Dense(1, activation='sigmoid', name='policy3')(x)\n",
    "\t\t#x = keras.layers.Dense(self.num_actions, activation='swish', name='policy_output')(x)\n",
    "\t\tx = keras.layers.Lambda(lambda x: (np.array(action_max-action_min) * (x)+action_min))(x)            #注意这里，先用tanh把范围限定在[-1,1]之间，再进行映射\n",
    "\n",
    "\t\tnet = keras.Model(inputs=[inputs], outputs=[x], name='Actor' + name)\n",
    "\t\treturn net\n",
    "\n",
    "\tdef self_act(self, x):\n",
    "\t\t#print(x)\n",
    "\t\treturn tf.clip_by_value(x, action_min, action_max)  #1/(1+tf.math.exp(-x))\n",
    "\n",
    "# def forward(self, state):\n",
    "#     #x = self.linear0(state)\n",
    "#     x = self.linear1(state)\n",
    "#     x = self.linear2(x)\n",
    "#     x = self.linear3(x)\n",
    "#\n",
    "#     output = self.output_linear(x)  # unit range output [-1, 1]\n",
    "#\n",
    "#     return output\n",
    "\n",
    "# def evaluate(self, state, eval_noise_scale):\n",
    "#     '''\n",
    "#     generate action with state for calculating gradients;\n",
    "#     eval_noise_scale: as the trick of target policy smoothing, for generating noisy actions.\n",
    "#     '''\n",
    "#     state = state.astype(np.float32)        #状态的type整理\n",
    "#     action = self.forward(state)            #通过state计算action，注意这里action范围是[-1,1]\n",
    "#\n",
    "#     #action = self.action_range * action     #映射到游戏的action取值范围\n",
    "#\n",
    "#     # add noise\n",
    "#     # normal = Normal(0, 1)                   #建立一个正态分布\n",
    "#     # eval_noise_clip = 2 * eval_noise_scale  #对噪声进行上下限裁剪。eval_noise_scale\n",
    "#     # noise = normal.sample(action.shape) * eval_noise_scale      #弄个一个noisy和action的shape一致，然后乘以scale\n",
    "#     # noise = tf.clip_by_value(noise, -eval_noise_clip, eval_noise_clip)  #对noisy进行剪切，不要太大也不要太小\n",
    "#     # action = action + noise                 #action加上噪音\n",
    "#\n",
    "#     return action\n",
    "\n",
    "#输入state，输出action\n",
    "def get_action(self, state, explore_noise_scale):\n",
    "    ''' generate action with state for interaction with envronment '''\n",
    "    action = self.net.predict(state)         #这里的forward函数，就是输入state，然后通过state输出action。只不过形式不一样而已。最后的激活函数式tanh，所以范围是[-1, 1]\n",
    "    #print(action,type(action))\n",
    "    action = action.numpy()[0]              #获得的action变成矩阵。\n",
    "\n",
    "    # add noise\n",
    "    # normal = Normal(0, 1)                   #生成normal这样一个正态分布\n",
    "    # noise = normal.sample(action.shape) * explore_noise_scale       #在正态分布中抽样一个和action一样shape的数据，然后乘以scale\n",
    "    # action = self.action_range * action + noise     #action乘以动作的范围，加上noise\n",
    "\n",
    "    return action\n",
    "\n",
    "def sample_action(self):\n",
    "    ''' generate random actions for exploration '''\n",
    "    a = tf.random.uniform([self.num_actions], action_min,action_max)\n",
    "\n",
    "    return a.numpy()\n",
    "\n",
    "\n",
    "class TD3_Trainer():\n",
    "\n",
    "\tdef __init__(\n",
    "\t\t\tself, replay_buffer, hidden_dim, action_range, policy_target_update_interval=1, q_lr=3e-4, policy_lr=3e-4\n",
    "\t):\n",
    "\t\tself.replay_buffer = replay_buffer\n",
    "\n",
    "\t\t# initialize all networks\n",
    "\t\t# 用两个Qnet来估算，doubleDQN的想法。同时也有两个对应的target_q_net\n",
    "\t\tself.q_net1 = QNetwork(state_dim, action_dim, hidden_dim).net\n",
    "\t\tself.q_net2 = QNetwork(state_dim, action_dim, hidden_dim).net\n",
    "\t\tself.target_q_net1 = QNetwork(state_dim, action_dim, hidden_dim).net\n",
    "\t\tself.target_q_net2 = QNetwork(state_dim, action_dim, hidden_dim).net\n",
    "\t\tself.policy_net = PolicyNetwork(state_dim, action_dim, hidden_dim, action_range).net\n",
    "\t\tself.target_policy_net = PolicyNetwork(state_dim, action_dim, hidden_dim, action_range).net\n",
    "\t\tprint('Q Network (1,2): ', self.q_net1)\n",
    "\t\tprint('Policy Network: ', self.policy_net)\n",
    "\n",
    "\t\t# initialize weights of target networks\n",
    "\t\t# 把net 赋值给target_network\n",
    "\t\tself.target_q_net1.set_weights(self.q_net1.get_weights())  #= self.target_ini(self.q_net1, self.target_q_net1)\n",
    "\t\tself.target_q_net2.set_weights(self.q_net2.get_weights())  # = self.target_ini(self.q_net2, self.target_q_net2)\n",
    "\t\tself.target_policy_net.set_weights(\n",
    "\t\t\tself.policy_net.get_weights())  # = self.target_ini(self.policy_net, self.target_policy_net)\n",
    "\n",
    "\t\tself.update_cnt = 0  #更新次数\n",
    "\t\tself.policy_target_update_interval = policy_target_update_interval  #策略网络更新频率\n",
    "\n",
    "\t\tself.q_optimizer1 = tf.optimizers.Adam(q_lr)\n",
    "\t\tself.q_optimizer2 = tf.optimizers.Adam(q_lr)\n",
    "\t\tself.policy_optimizer = tf.optimizers.Adam(policy_lr)\n",
    "\n",
    "\t#在网络初始化的时候进行硬更新\n",
    "\tdef target_ini(self, net, target_net):\n",
    "\t\t''' hard-copy update for initializing target networks '''\n",
    "\t\tfor target_param, param in zip(target_net.trainable_weights, net.trainable_weights):\n",
    "\t\t\ttarget_param.assign(param)\n",
    "\t\treturn target_net\n",
    "\n",
    "\t#在更新的时候进行软更新\n",
    "\tdef target_soft_update(self, net, target_net, soft_tau):\n",
    "\t\t''' soft update the target net with Polyak averaging '''\n",
    "\t\tfor target_param, param in zip(target_net.trainable_weights, net.trainable_weights):\n",
    "\t\t\ttarget_param.assign(  # copy weight value into target parameters\n",
    "\t\t\t\ttarget_param * (1.0 - soft_tau) + param * soft_tau\n",
    "\t\t\t\t# 原来参数占比 + 目前参数占比\n",
    "\t\t\t)\n",
    "\t\treturn target_net\n",
    "\n",
    "\tdef update(self, batch_size, eval_noise_scale, reward_scale=10., gamma=0.9, soft_tau=1e-3):\n",
    "\t\t''' update all networks in TD3 '''\n",
    "\t\tself.update_cnt += 1  #计算更新次数\n",
    "\t\tstate, action, reward, next_state = self.replay_buffer.sample(batch_size)  #从buffer sample数据\n",
    "\n",
    "\t\treward = reward[:, np.newaxis]  # expand dim， 调整形状，方便输入网络\n",
    "\n",
    "\t\t# 输入s',从target_policy_net计算a'。注意这里有加noisy的\n",
    "\t\t##改：此处为不加噪音，后续可与加入噪音进行对比\n",
    "\t\tnew_next_action = self.target_policy_net.predict(next_state)  # clipped normal noise#到底裁不裁？？？？？？？？？？？？？？？？？？？\n",
    "\n",
    "\t\t# 归一化reward.(有正有负)\n",
    "\t\t# reward = reward_scale * (reward - np.mean(reward, axis=0)) / (np.std(reward, axis=0) + 1e-6)  # normalize with batch mean and std; plus a small number to prevent numerical problem\n",
    "\n",
    "\t\t# Training Q Function\n",
    "\t\t# 把s'和a'堆叠在一起，一起输入到target_q_net。\n",
    "\t\t# 有两个qnet，我们取最小值\n",
    "\n",
    "\t\ttarget_q_input = tf.concat([next_state, new_next_action], 1)  # the dim 0 is number of samples\n",
    "\t\ttarget_q_min = tf.minimum(self.target_q_net1.predict(target_q_input),\n",
    "\t\t\t\t\t\t\t\t  self.target_q_net2.predict(target_q_input))\n",
    "\n",
    "\t\t#计算target_q的值，用于更新q_net\n",
    "\t\t#之前有把done从布尔变量改为int，就是为了这里能够直接计算。\n",
    "\t\ttarget_q_value = reward + gamma * target_q_min  # if done==1, only reward\n",
    "\t\t#action = action[:, np.newaxis]\n",
    "\t\tstate = state.astype('float32')\n",
    "\t\taction = action.astype('float32')\n",
    "\t\tq_input = tf.concat([state, action], 1)  # input of q_net\n",
    "\n",
    "\t\tq_input = tf.dtypes.cast(q_input, tf.float32)\n",
    "\t\t#更新q_net1\n",
    "\t\t#这里其实和DQN是一样的\n",
    "\t\twith tf.GradientTape() as q1_tape:\n",
    "\t\t\tpredicted_q_value1 = self.q_net1(q_input)\n",
    "\t\t\tq_value_loss1 = tf.reduce_mean(tf.square(predicted_q_value1 - target_q_value))\n",
    "\t\tq1_grad = q1_tape.gradient(q_value_loss1, self.q_net1.trainable_weights)\n",
    "\t\tself.q_optimizer1.apply_gradients(zip(q1_grad, self.q_net1.trainable_weights))\n",
    "\n",
    "\t\t#更新q_net2\n",
    "\t\twith tf.GradientTape() as q2_tape:\n",
    "\t\t\tpredicted_q_value2 = self.q_net2(q_input)\n",
    "\t\t\tq_value_loss2 = tf.reduce_mean(tf.square(predicted_q_value2 - target_q_value))\n",
    "\t\tq2_grad = q2_tape.gradient(q_value_loss2, self.q_net2.trainable_weights)\n",
    "\t\tself.q_optimizer2.apply_gradients(zip(q2_grad, self.q_net2.trainable_weights))\n",
    "\n",
    "\t\t# Training Policy Function\n",
    "\t\t# policy不是经常updata的，而qnet更新一定次数，才updata一次\n",
    "\t\tpolicy_loss_rec = False\n",
    "\t\tif self.update_cnt % self.policy_target_update_interval == 0:\n",
    "\t\t\t#更新policy_net\n",
    "\t\t\twith tf.GradientTape() as p_tape:\n",
    "\t\t\t\t# 计算 action = Policy(s)，注意这里是没有noise的\n",
    "\t\t\t\tnew_action = self.policy_net(\n",
    "\t\t\t\t\tstate\n",
    "\t\t\t\t)  # no noise, deterministic policy gradients\n",
    "\n",
    "\t\t\t\t#叠加state和action\n",
    "\t\t\t\tnew_q_input = tf.concat([state, new_action], 1)\n",
    "\t\t\t\t# ''' implementation 1 '''\n",
    "\t\t\t\t# predicted_new_q_value = tf.minimum(self.q_net1(new_q_input),self.q_net2(new_q_input))\n",
    "\t\t\t\t''' implementation 2 '''\n",
    "\t\t\t\tpredicted_new_q_value = self.q_net1(new_q_input)\n",
    "\t\t\t\tpolicy_loss = -tf.reduce_mean(predicted_new_q_value)  #梯度上升\n",
    "\t\t\t\tpolicy_loss_rec = policy_loss\n",
    "\t\t\tp_grad = p_tape.gradient(policy_loss, self.policy_net.trainable_weights)\n",
    "\t\t\tself.policy_optimizer.apply_gradients(zip(p_grad, self.policy_net.trainable_weights))\n",
    "\t\t\t#print(self.policy_net.)\n",
    "\n",
    "\t\t\t# Soft update the target nets\n",
    "\t\t\t# 软更新target_network三个\n",
    "\t\t\tself.target_q_net1 = self.target_soft_update(self.q_net1, self.target_q_net1, soft_tau)\n",
    "\t\t\tself.target_q_net2 = self.target_soft_update(self.q_net2, self.target_q_net2, soft_tau)\n",
    "\t\t\tself.target_policy_net = self.target_soft_update(self.policy_net, self.target_policy_net, soft_tau)\n",
    "\n",
    "\t\tif policy_loss_rec:\n",
    "\t\t\treturn q_value_loss1, q_value_loss2, policy_loss_rec\n",
    "\t\telse:\n",
    "\t\t\treturn q_value_loss1, q_value_loss2, 10086\n",
    "\n",
    "\tdef save_weights(self, model_path, describe=''):  # save trained weights\n",
    "\t\tsave_path = os.path.join(model_path, describe)\n",
    "\t\tif not os.path.exists(save_path):\n",
    "\t\t\tos.makedirs(save_path)\n",
    "\t\tself.q_net1.save(save_path + '/model_q_net1.hdf5')\n",
    "\t\tself.q_net2.save(save_path + '/model_q_net2.hdf5')\n",
    "\t\tself.target_q_net1.save(save_path + '/model_target_q_net1.hdf5')\n",
    "\t\tself.target_q_net2.save(save_path + '/model_target_q_net2.hdf5')\n",
    "\t\tself.policy_net.save(save_path + '/model_policy_net.hdf5')\n",
    "\t\tself.target_policy_net.save(save_path + '/model_target_policy_net.hdf5')\n",
    "\t\treturn save_path\n",
    "\n",
    "\t# tl.files.save_npz(self.q_net1.trainable_weights, name='model_q_net1.npz')\n",
    "\t# tl.files.save_npz(self.q_net2.trainable_weights, name='model_q_net2.npz')\n",
    "\t# tl.files.save_npz(self.target_q_net1.trainable_weights, name='model_target_q_net1.npz')\n",
    "\t# tl.files.save_npz(self.target_q_net2.trainable_weights, name='model_target_q_net2.npz')\n",
    "\t# tl.files.save_npz(self.policy_net.trainable_weights, name='model_policy_net.npz')\n",
    "\t# tl.files.save_npz(self.target_policy_net.trainable_weights, name='model_target_policy_net.npz')\n",
    "\n",
    "\tdef load_weights(self, save_path):  # load trained weights\n",
    "\t\tself.q_net1 = keras.models.load_model(save_path + '/model_q_net1.hdf5')\n",
    "\t\tself.q_net2 = keras.models.load_model(save_path + '/model_q_net2.hdf5')\n",
    "\t\tself.target_q_net1 = keras.models.load_model(save_path + '/model_target_q_net1.hdf5')\n",
    "\t\tself.target_q_net2 = keras.models.load_model(save_path + '/model_target_q_net2.hdf5')\n",
    "\t\tself.policy_net = keras.models.load_model(save_path + '/model_policy_net.hdf5')\n",
    "\t\tself.target_policy_net = keras.models.load_model(save_path + '/model_target_policy_net.hdf5')\n",
    "\t# tl.files.load_and_assign_npz(name='model_q_net1.npz', network=self.q_net1)\n",
    "\t# tl.files.load_and_assign_npz(name='model_q_net2.npz', network=self.q_net2)\n",
    "\t# tl.files.load_and_assign_npz(name='model_target_q_net1.npz', network=self.target_q_net1)\n",
    "\t# tl.files.load_and_assign_npz(name='model_target_q_net2.npz', network=self.target_q_net2)\n",
    "\t# tl.files.load_and_assign_npz(name='model_policy_net.npz', network=self.policy_net)\n",
    "\t# tl.files.load_and_assign_npz(name='model_target_policy_net.npz', network=self.target_policy_net)\n",
    "\n",
    "from tqdm import tqdm\n",
    "env = SGTR_env(model=model,set_point=-56,train_datasets=train_datasets,mean=mean,std=std)               #环境\n",
    "action_dim = env.action_space.shape[0]      #动作空间\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "# 初始化缓冲区\n",
    "replay_buffer = ReplayBuffer(replay_buffer_size)\n",
    "\n",
    "# 初始化agent\n",
    "td3_trainer=TD3_Trainer(replay_buffer, hidden_dim=hidden_dim, policy_target_update_interval=policy_target_update_interval, action_range=action_range, q_lr=q_lr, policy_lr=policy_lr )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 542/1000 [01:46<01:29,  5.10it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2100\\183184330.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# 与环境进行交互\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2100\\39223578.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[1;31m#model(test_input, training=False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[0mnext_variable_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m             \u001b[0mnext_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mzip_state_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnext_variable_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "frame_idx = 0                           #总步数\n",
    "rewards = []                            #记录每个EP的总reward\n",
    "t0 = time.time()\n",
    "while frame_idx < max_frames:           #小于最大步数，就继续训练\n",
    "    state = env.reset()                 #初始化state\n",
    "    state = state.astype(np.float32)    #整理state的类型\n",
    "    episode_reward = 0\n",
    "\n",
    "\n",
    "    # 开始训练\n",
    "    for step in tqdm(range(max_steps)):\n",
    "        if frame_idx > explore_steps:       #如果小于500步，就随机，如果大于就用get-action\n",
    "            action = td3_trainer.policy_net.predict(state).numpy()[0]+tf.random.normal(1)    #带有noisy的action\n",
    "        else:\n",
    "            action = tf.random.uniform([1], action_min,action_max)\n",
    "\n",
    "        # 与环境进行交互\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = next_state.astype(np.float32)\n",
    "        action = np.array(action).tolist()\n",
    "        done = 1 if done ==True else 0\n",
    "\n",
    "        #记录数据在replay_buffer\n",
    "        replay_buffer.push(state, action, reward, next_state)\n",
    "\n",
    "        #赋值state，累计总reward，步数\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "\n",
    "        #如果数据超过一个batch_size的大小，那么就开始更新\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            for i in range(update_itr):         #注意：这里更新可以更新多次！\n",
    "                td3_trainer.update(batch_size, eval_noise_scale=0.1, reward_scale=0.1)\n",
    "\n",
    "\n",
    "        # if done:\n",
    "        #     break\n",
    "    frame_idx+=1\n",
    "    episode=frame_idx  # current episode\n",
    "    all_episodes = max_frames # total episodes\n",
    "    print(\n",
    "            '\\rEpisode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}| State: {}'.format(\n",
    "                episode, all_episodes, episode_reward,\n",
    "                time.time()-t0,state\n",
    "            ), end=''\n",
    "        )\n",
    "\n",
    "\n",
    "    # print(episode_reward,'###',state)\n",
    "    rewards.append(episode_reward)\n",
    "plt.plot(rewards)\n",
    "plt.show()\n",
    "td3_trainer.save_weights()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RLcontrrol",
   "language": "python",
   "name": "rlcontrrol"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
