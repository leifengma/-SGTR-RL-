{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "array([7.73895844e+00, 2.61572654e+02, 2.63953474e+02, 4.56612677e+00,\n       3.53780148e+01, 1.76500712e+02, 2.62570725e+01, 1.01088743e+02,\n       1.06785439e+00, 2.46672616e-02, 8.40632707e-01, 1.46415121e+01])"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os,time\n",
    "\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.layers import Activation\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import sys\n",
    "\n",
    "sys.path.append (\"..\")\n",
    "from LSTM模型 import keras_model  # 导入\n",
    "model = keras.models.load_model(r'M:\\work\\project_program\\bishe\\pyproject\\RL_pid_control_critic_actor\\强化学习+Lstm\\一回路温度控制\\best_model.hdf5')\n",
    "\n",
    "data_path=r'M:\\work\\project_program\\bishe\\pyproject\\RL_pid_control_critic_actor\\强化学习+Lstm\\一回路温度控制\\一回路温度train数据.npy'\n",
    "train_data=np.load(data_path,allow_pickle=True)\n",
    "train_data=train_data.astype('float64')\n",
    "men_std=np.load(\"一回路温度mean,std数据.npy\",allow_pickle=True)\n",
    "men_std[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "fff=pd.DataFrame()\n",
    "data=[]\n",
    "for i in range(10000):\n",
    "\taaa = float(tf.clip_by_value(tf.random.normal([1],-1.1,3.5),-1.11,6.5))\n",
    "\tdata.append(aaa)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot:ylabel='Frequency'>"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fff[\"data\"]=data\n",
    "fff.plot.hist(bins=13, alpha=0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot:ylabel='Frequency'>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fff=pd.DataFrame()\n",
    "data=[]\n",
    "for i in range(10000):\n",
    "\taaa = float(tf.random.normal([1],-5,1))\n",
    "\tdata.append(aaa)\n",
    "\n",
    "fff[\"data\"]=data\n",
    "fff.plot.hist(bins=13, alpha=0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 2.69183203,  1.91655899,  2.35629792,  1.24940039, -1.86650263,\n        -3.12581857, -1.69267536, -3.03596508, -1.12381861, -1.38653583,\n        -1.68733414, -1.11451727],\n       [ 2.69102277,  1.91737342,  2.35612837,  1.24941672, -1.86650263,\n        -3.12581857, -1.69696042, -3.03596508, -1.12272887, -1.38675354,\n        -1.6865283 , -1.11451727],\n       [ 2.68838387,  1.9172741 ,  2.35599273,  1.24944789, -1.86650263,\n        -3.12581857, -1.70177133, -3.03596508, -1.12142411, -1.38725116,\n        -1.68565531, -1.11451727],\n       [ 2.68535794,  1.91709532,  2.35585709,  1.2494954 , -1.86650263,\n        -3.12581857, -1.7051949 , -3.03596508, -1.12003997, -1.38717341,\n        -1.68484948, -1.11451727],\n       [ 2.68338756,  1.91713505,  2.35572145,  1.24955032, -1.86650263,\n        -3.12581857, -1.70810382, -3.03596508, -1.11863801, -1.38669133,\n        -1.68404364, -1.11451727],\n       [ 2.68148755,  1.91723437,  2.35563668,  1.24961267, -1.86650263,\n        -3.12581857, -1.71092324, -3.03596508, -1.11723506, -1.38652027,\n        -1.6832378 , -1.11451727],\n       [ 2.67948199,  1.91729396,  2.3555519 ,  1.24967799, -1.86650263,\n        -3.12581857, -1.71378741, -3.03596508, -1.11583903, -1.3869246 ,\n        -1.68243197, -1.11451727],\n       [ 2.67775791,  1.91739328,  2.35550104,  1.24974331, -1.86650263,\n        -3.12581857, -1.71633271, -3.03596508, -1.11445493, -1.38729782,\n        -1.68162613, -1.11451727],\n       [ 2.67628013,  1.91747274,  2.35545017,  1.24980863, -1.86650263,\n        -3.12581857, -1.71867104, -3.03596508, -1.11309008, -1.3869868 ,\n        -1.6808203 , -1.11451727],\n       [ 2.67494308,  1.91753233,  2.35543322,  1.24987395, -1.86650263,\n        -3.12581857, -1.72077441, -3.03596508, -1.11175322, -1.38655138,\n        -1.67994731, -1.11451727]])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_str_time ():\n",
    "\ts = '_'\n",
    "\tfor i in time.localtime (time.time ())[0:6]:\n",
    "\t\ts += str (i) + '_'\n",
    "\treturn s\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SGTR_env ():\n",
    "\tdef __init__ (self,model,set_point,train_datasets,mean,std):\n",
    "\t\tself.action_space = np.array ([0] * 1)\n",
    "\t\tself.observation_space = np.array ([0] * 11)\n",
    "\t\tself.mean = mean\n",
    "\t\tself.std = std\n",
    "\t\tself.response = []\n",
    "\t\tself.set_point = set_point\n",
    "\t\tself.train_datasets = train_datasets\n",
    "\t\tself.model = model\n",
    "\t\tself.state = self.train_datasets[random.randint (0,train_datasets.shape[0])]\n",
    "\t\tself.step_count = 0  # 步数计数\n",
    "\n",
    "\tdef reset (self):\n",
    "\t\tself.step_count = 0\n",
    "\t\t# begin_index = [2500,6500,11000,14500]\n",
    "\t\tbegin_index = range(1800, self.train_datasets.shape[0]-3000,100)\n",
    "\t\tstate = self.train_datasets[random.sample(begin_index,1)[0]]  # 10420]\n",
    "\t\t#  state = self.train_datasets[random.randint(0, self.train_datasets.shape[0]-1)]\n",
    "\t\tself.state = np.array (state)\n",
    "\t\treturn np.array (state)\n",
    "\n",
    "\tdef cal_origin_val (self,pos,now_val):\n",
    "\t\t\"\"\"\n",
    "\t\t计算未归一化的值\n",
    "\t\t\"\"\"\n",
    "\t\tval = now_val * self.std[pos] + self.mean[pos]\n",
    "\t\treturn val\n",
    "\n",
    "\tdef justice_down (self,next_state,step):\n",
    "\t\t\"\"\"\n",
    "\t\t判断是否达到失败条件，deltaT<10或70分钟内未能实现一二回路压力平衡（小于1MP）\n",
    "\t\t\"\"\"\n",
    "\t\tori_deltaT = self.cal_origin_val (6,next_state[-1,6])\n",
    "\t\t# ori_pressure = self.cal_origin_val(0,next_state[-1, 0])\n",
    "\t\tif ori_deltaT < 10:  # or (step>4200 and ori_pressure<1):\n",
    "\t\t\treturn True\n",
    "\n",
    "\t\telse:\n",
    "\t\t\treturn False\n",
    "\n",
    "\tdef cal_reward (self,speed):\n",
    "\t\t# if speed < 29.276 and speed > -141.276:\n",
    "\t\t# \treward = -0.001 * speed ** 2 - 0.112 * speed + 4.136\n",
    "\t\t# elif speed < -141.276:\n",
    "\t\t# \treward = 0.01 * speed + 1.2\n",
    "\t\t# else:\n",
    "\t\t# \treward = -0.2 * speed + 6\n",
    "\t\tif speed <= -56:\n",
    "\t\t\tmu,sigma = -56,5\n",
    "\t\t\treward = (1 / (math.sqrt (2 * math.pi) * sigma) * math.e ** (\n",
    "\t\t\t\t\t\t-(speed - mu) ** 2 / (2 * sigma ** 2))) * 37 - 0.5\n",
    "\n",
    "\t\telif -10 >= speed > -56:\n",
    "\t\t\treward = (-1 / 23) * speed + (1 / 2 - 10 / 23)\n",
    "\t\telif speed > -10:\n",
    "\t\t\treward = (-1 / 20) * speed  # +(19/42)\n",
    "\t\telse:\n",
    "\t\t\tprint (\"计算奖励错误\")\n",
    "\n",
    "\t\treturn reward\n",
    "\n",
    "\tdef step (self,action):\n",
    "\t\tself.step_count += 1\n",
    "\t\tself.state[-1,-1] = action\n",
    "\t\t# model(test_input, training=False)\n",
    "\t\tnext_variable_state = np.array (self.model (np.array ([self.state]),training=False))\n",
    "\t\tnext_action = action\n",
    "\t\tzip_state_action = np.append (next_variable_state,next_action).reshape (1,-1)\n",
    "\t\tnext_state = np.row_stack ((self.state,zip_state_action))\n",
    "\t\tnext_state = np.delete (next_state,0,axis=0)\n",
    "\t\tori_temp_last = self.cal_origin_val (1,next_state[-1,1])\n",
    "\t\tori_temp_before_last = self.cal_origin_val (1,next_state[-2,1])\n",
    "\t\ttemp_change_speed = (ori_temp_last - ori_temp_before_last) * 3600\n",
    "\t\treward = self.cal_reward (temp_change_speed)\n",
    "\t\tdone = self.justice_down (next_state,self.step_count)\n",
    "\t\tself.state = next_state\n",
    "\n",
    "\t\treturn next_state,reward,done,{}\n",
    "\n",
    "\n",
    "# import tensorflow_probability as tfp\n",
    "# import tensorlayer as tl\n",
    "#\n",
    "# tfd = tfp.distributions\n",
    "# Normal = tfd.Normal\n",
    "#\n",
    "# tl.logging.set_verbosity(tl.logging.DEBUG)\n",
    "\n",
    "# random.seed (2)\n",
    "# np.random.seed (2)\n",
    "# tf.random.set_seed (2)  # reproducible\n",
    "\n",
    "#####################  hyper parameters  ####################\n",
    "# choose env\n",
    "ENV = 'Pendulum-v0'\n",
    "action_range = 1.  # scale action, [-action_range, action_range]\n",
    "\n",
    "# RL training\n",
    "max_frames = 4e6  # total number of steps for training\n",
    "test_frames = 300  # total number of steps for testing\n",
    "max_steps = 2000  # maximum number of steps for one episode\n",
    "batch_size = 64  # udpate batchsize\n",
    "explore_frams = 100  # 500 for random action sampling in the beginning of training\n",
    "update_itr = 3  # repeated updates for single step\n",
    "hidden_dim = 32  # size of hidden layers for networks\n",
    "q_lr = 3e-4  # q_net learning rate\n",
    "policy_lr = 3e-4  # policy_net learning rate\n",
    "policy_target_update_interval = 3  # delayed steps for updating the policy network and target networks\n",
    "explore_noise_scale = 1.0  # range of action noise for exploration\n",
    "eval_noise_scale = 0.5  # range of action noise for evaluation of action value\n",
    "reward_scale = 1.  # value range of reward\n",
    "replay_buffer_size = 30000  # size of replay buffer\n",
    "\n",
    "\n",
    "###############################  TD3  ####################################\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "\t'''\n",
    "\ta ring buffer for storing transitions and sampling for training\n",
    "\t:state: (state_dim,)\n",
    "\t:action: (action_dim,)\n",
    "\t:reward: (,), scalar\n",
    "\t:next_state: (state_dim,)\n",
    "\t:done: (,), scalar (0 and 1) or bool (True and False)\n",
    "\t'''\n",
    "\n",
    "\tdef __init__ (self,capacity):\n",
    "\t\tself.capacity = capacity  # buffer的最大值\n",
    "\t\tself.buffer = []  # buffer列表\n",
    "\t\tself.position = 0  # 当前输入的位置，相当于指针\n",
    "\n",
    "\tdef push (self,state,action,reward,next_state):\n",
    "\t\t# 如果buffer的长度小于最大值，也就是说，第一环的时候，需要先初始化一个“空间”，这个空间值为None，再给这个空间赋值。\n",
    "\t\tif len (self.buffer) < self.capacity:\n",
    "\t\t\tself.buffer.append (None)\n",
    "\t\tself.buffer[self.position] = (state,action,reward,next_state)\n",
    "\t\tself.position = int ((self.position + 1) % self.capacity)  # as a ring buffer\n",
    "\n",
    "\tdef sample (self,batch_size):\n",
    "\t\tbatch = random.sample (self.buffer,batch_size)\n",
    "\t\tstate,action,reward,next_state = map (np.stack,zip (*batch))  # stack for each element\n",
    "\t\t'''\n",
    "\t\tthe * serves as unpack: sum(a,b) <=> batch=(a,b), sum(*batch) ;\n",
    "\t\tzip: a=[1,2], b=[2,3], zip(a,b) => [(1, 2), (2, 3)] ;\n",
    "\t\tthe map serves as mapping the function on each list element: map(square, [2,3]) => [4,9] ;\n",
    "\t\tnp.stack((1,2)) => array([1, 2])\n",
    "\t\t'''\n",
    "\t\treturn state,action,reward,next_state\n",
    "\n",
    "\tdef __len__ (self):\n",
    "\t\treturn len (self.buffer)\n",
    "\n",
    "\n",
    "# 在代码中没有用到，但我们可以学习下，这里是直接修改gym环境的动作输出，把输出归一化。\n",
    "# class NormalizedActions(gym.ActionWrapper):\n",
    "# \t''' normalize the actions to be in reasonable range '''\n",
    "#\n",
    "# \tdef _action(self, action):\n",
    "# \t\tlow = self.action_space.low  #动作空间最小值\n",
    "# \t\thigh = self.action_space.high  #动作空间最大值\n",
    "#\n",
    "# \t\taction = low + (action + 1.0) * 0.5 * (high - low)\n",
    "# \t\taction = np.clip(action, low, high)\n",
    "#\n",
    "# \t\treturn action\n",
    "#\n",
    "# \tdef _reverse_action(self, action):\n",
    "# \t\tlow = self.action_space.low\n",
    "# \t\thigh = self.action_space.high\n",
    "#\n",
    "# \t\taction = 2 * (action - low) / (high - low) - 1\n",
    "# \t\taction = np.clip(action, low, high)\n",
    "#\n",
    "# \t\treturn action\n",
    "\n",
    "\n",
    "class QNetwork ():\n",
    "\t''' the network for evaluate values of state-action pairs: Q(s,a) '''\n",
    "\n",
    "\tdef __init__ (self,num_inputs,num_actions,hidden_dim,init_w = 3e-3):\n",
    "\t\tsuper (QNetwork,self).__init__ ()\n",
    "\t\tself.input_dim = num_inputs + num_actions\n",
    "\t\tself.net = self.get_net ()\n",
    "\n",
    "\t# w_init = tf.keras.initializers.glorot_normal(seed=None)\n",
    "\t# w_init = tf.random_uniform_initializer(-init_w, init_w)\n",
    "\t#\n",
    "\t# self.linear1 = Dense(n_units=hidden_dim, act=tf.nn.relu, W_init=w_init, in_channels=input_dim, name='q1')\n",
    "\t# self.linear2 = Dense(n_units=hidden_dim, act=tf.nn.relu, W_init=w_init, in_channels=hidden_dim, name='q2')\n",
    "\t# self.linear3 = Dense(n_units=1, W_init=w_init, in_channels=hidden_dim, name='q3')\n",
    "\n",
    "\tdef get_net (self,name = 'Q_model'):\n",
    "\t\tinputs = keras.layers.Input (shape=[self.input_dim],name='C_input')\n",
    "\t\tx = keras.layers.Dense (64,activation='tanh',name='C_l1') (inputs)\n",
    "\t\t# x = keras.layers.BatchNormalization()(x)\n",
    "\t\tx = keras.layers.Dense (32,activation='tanh',name='C_l2') (x)\n",
    "\t\t# x = keras.layers.BatchNormalization()(x)\n",
    "\t\tx = keras.layers.Dense (1,name='C_out') (x)\n",
    "\t\tnet = keras.Model (inputs=[inputs],outputs=[x],name='Critic' + name)\n",
    "\t\treturn net\n",
    "\n",
    "\n",
    "# def forward(self, input):\n",
    "#     x = self.tanh1(input)\n",
    "#     x = self.tanh2(x)\n",
    "#     x = self.tanh3(x)\n",
    "#     return x\n",
    "\n",
    "\n",
    "class PolicyNetwork ():\n",
    "\t''' the network for generating non-determinstic (Gaussian distributed) action from the state input '''\n",
    "\n",
    "\tdef __init__ (self,num_inputs,num_actions,hidden_dim,action_range = 1.,init_w = 3e-3):\n",
    "\t\tsuper (PolicyNetwork,self).__init__ ()\n",
    "\t\tget_custom_objects ().update ({'swish': Activation (self.self_act)})\n",
    "\n",
    "\t\tself.num_inputs = num_inputs\n",
    "\t\tself.action_range = action_range\n",
    "\t\tself.num_actions = num_actions\n",
    "\t\tself.net = self.get_net ()\n",
    "\n",
    "\t# w_init = tf.keras.initializers.glorot_normal(seed=None)\n",
    "\t# w_init = tf.random_uniform_initializer(-init_w, init_w)\n",
    "\n",
    "\t# self.tanh1 = keras.layers.Dense(hidden_dim, activation='rtanh',\n",
    "\t#                                   name='policy1')  #Dense(n_units=hidden_dim, act=tf.nn.rtanh, W_init=w_init, in_channels=num_inputs, name='policy1')\n",
    "\t# self.tanh2 = keras.layers.Dense(hidden_dim, activation='rtanh',\n",
    "\t#                                   name='policy2')  #Dense(n_units=hidden_dim, act=tf.nn.rtanh, W_init=w_init, in_channels=hidden_dim, name='policy2')\n",
    "\t# self.tanh3 = keras.layers.Dense(hidden_dim, activation='rtanh',\n",
    "\t#                                   name='policy3')  #Dense(n_units=hidden_dim, act=tf.nn.rtanh, W_init=w_init, in_channels=hidden_dim, name='policy3')\n",
    "\t#\n",
    "\t# self.output_tanh = keras.layers.Dense(num_actions,activation='tanh',\n",
    "\t#                                         name='policy_output')  #Dense(n_units=num_actions, W_init=w_init, b_init=tf.random_uniform_initializer(-init_w, init_w), in_channels=hidden_dim, name='policy_output')\n",
    "\n",
    "\tdef get_net (self,name = 'policy_model'):\n",
    "\t\t\"\"\"\n",
    "\t\tBuild actor network\n",
    "\t\t:param input_state_shape: state\n",
    "\t\t:param name: name\n",
    "\t\t:return: act\n",
    "\t\t\"\"\"\n",
    "\t\tinputs = keras.layers.Input (shape=[self.num_inputs],name='A_input')\n",
    "\t\t# x = keras.layers.BatchNormalization()(inputs)\n",
    "\t\tx = keras.layers.Dense (64,activation='tanh',name='policy1') (inputs)\n",
    "\t\t# x = keras.layers.BatchNormalization()(x)\n",
    "\t\tx = keras.layers.Dense (32,activation='tanh',name='policy2') (x)\n",
    "\t\t# x = keras.layers.BatchNormalization()(x)\n",
    "\t\tx = keras.layers.Dense (1,activation='sigmoid',name='policy3') (x)\n",
    "\t\t# x = keras.layers.Dense(self.num_actions, activation='swish', name='policy_output')(x)\n",
    "\t\tx = keras.layers.Lambda (lambda x: (np.array (action_max - action_min) * (x) + action_min)) (\n",
    "\t\t\tx)  # 注意这里，先用tanh把范围限定在[-1,1]之间，再进行映射\n",
    "\n",
    "\t\tnet = keras.Model (inputs=[inputs],outputs=[x],name='Actor' + name)\n",
    "\t\treturn net\n",
    "\n",
    "\tdef self_act (self,x):\n",
    "\t\t# print(x)\n",
    "\t\treturn tf.clip_by_value (x,action_min,action_max)  # 1/(1+tf.math.exp(-x))\n",
    "\n",
    "\n",
    "# def forward(self, state):\n",
    "#     #x = self.linear0(state)\n",
    "#     x = self.linear1(state)\n",
    "#     x = self.linear2(x)\n",
    "#     x = self.linear3(x)\n",
    "#\n",
    "#     output = self.output_linear(x)  # unit range output [-1, 1]\n",
    "#\n",
    "#     return output\n",
    "\n",
    "# def evaluate(self, state, eval_noise_scale):\n",
    "#     '''\n",
    "#     generate action with state for calculating gradients;\n",
    "#     eval_noise_scale: as the trick of target policy smoothing, for generating noisy actions.\n",
    "#     '''\n",
    "#     state = state.astype(np.float32)        #状态的type整理\n",
    "#     action = self.forward(state)            #通过state计算action，注意这里action范围是[-1,1]\n",
    "#\n",
    "#     #action = self.action_range * action     #映射到游戏的action取值范围\n",
    "#\n",
    "#     # add noise\n",
    "#     # normal = Normal(0, 1)                   #建立一个正态分布\n",
    "#     # eval_noise_clip = 2 * eval_noise_scale  #对噪声进行上下限裁剪。eval_noise_scale\n",
    "#     # noise = normal.sample(action.shape) * eval_noise_scale      #弄个一个noisy和action的shape一致，然后乘以scale\n",
    "#     # noise = tf.clip_by_value(noise, -eval_noise_clip, eval_noise_clip)  #对noisy进行剪切，不要太大也不要太小\n",
    "#     # action = action + noise                 #action加上噪音\n",
    "#\n",
    "#     return action\n",
    "\n",
    "# 输入state，输出action\n",
    "def get_action (self,state,explore_noise_scale):\n",
    "\t''' generate action with state for interaction with envronment '''\n",
    "\taction = self.net.predict (state)  # 这里的forward函数，就是输入state，然后通过state输出action。只不过形式不一样而已。最后的激活函数式tanh，所以范围是[-1, 1]\n",
    "\t# print(action,type(action))\n",
    "\taction = action.numpy ()[0]  # 获得的action变成矩阵。\n",
    "\n",
    "\t# add noise\n",
    "\t# normal = Normal(0, 1)                   #生成normal这样一个正态分布\n",
    "\t# noise = normal.sample(action.shape) * explore_noise_scale       #在正态分布中抽样一个和action一样shape的数据，然后乘以scale\n",
    "\t# action = self.action_range * action + noise     #action乘以动作的范围，加上noise\n",
    "\n",
    "\treturn action\n",
    "\n",
    "\n",
    "def sample_action (self):\n",
    "\t''' generate random actions for exploration '''\n",
    "\ta = tf.random.uniform ([self.num_actions],action_min,action_max)\n",
    "\n",
    "\treturn a.numpy ()\n",
    "\n",
    "\n",
    "class TD3_Trainer ():\n",
    "\n",
    "\tdef __init__ (\n",
    "\t\t\tself,replay_buffer,hidden_dim,action_range,policy_target_update_interval = 1,q_lr = 3e-4,policy_lr = 3e-4\n",
    "\t):\n",
    "\t\tself.replay_buffer = replay_buffer\n",
    "\n",
    "\t\t# initialize all networks\n",
    "\t\t# 用两个Qnet来估算，doubleDQN的想法。同时也有两个对应的target_q_net\n",
    "\t\tself.q_net1 = QNetwork (state_dim,action_dim,hidden_dim).net\n",
    "\t\tself.q_net2 = QNetwork (state_dim,action_dim,hidden_dim).net\n",
    "\t\tself.target_q_net1 = QNetwork (state_dim,action_dim,hidden_dim).net\n",
    "\t\tself.target_q_net2 = QNetwork (state_dim,action_dim,hidden_dim).net\n",
    "\t\tself.policy_net = PolicyNetwork (state_dim,action_dim,hidden_dim,action_range).net\n",
    "\t\tself.target_policy_net = PolicyNetwork (state_dim,action_dim,hidden_dim,action_range).net\n",
    "\t\tprint ('Q Network (1,2): ',self.q_net1)\n",
    "\t\tprint ('Policy Network: ',self.policy_net)\n",
    "\n",
    "\t\t# initialize weights of target networks\n",
    "\t\t# 把net 赋值给target_network\n",
    "\t\tself.target_q_net1.set_weights (\n",
    "\t\t\tself.q_net1.get_weights ())  # = self.target_ini(self.q_net1, self.target_q_net1)\n",
    "\t\tself.target_q_net2.set_weights (\n",
    "\t\t\tself.q_net2.get_weights ())  # = self.target_ini(self.q_net2, self.target_q_net2)\n",
    "\t\tself.target_policy_net.set_weights (\n",
    "\t\t\tself.policy_net.get_weights ())  # = self.target_ini(self.policy_net, self.target_policy_net)\n",
    "\n",
    "\t\tself.update_cnt = 0  # 更新次数\n",
    "\t\tself.policy_target_update_interval = policy_target_update_interval  # 策略网络更新频率\n",
    "\n",
    "\t\tself.q_optimizer1 = tf.optimizers.Adam (q_lr)\n",
    "\t\tself.q_optimizer2 = tf.optimizers.Adam (q_lr)\n",
    "\t\tself.policy_optimizer = tf.optimizers.Adam (policy_lr)\n",
    "\n",
    "\t# 在网络初始化的时候进行硬更新\n",
    "\tdef target_ini (self,net,target_net):\n",
    "\t\t''' hard-copy update for initializing target networks '''\n",
    "\t\tfor target_param,param in zip (target_net.trainable_weights,net.trainable_weights):\n",
    "\t\t\ttarget_param.assign (param)\n",
    "\t\treturn target_net\n",
    "\n",
    "\t# 在更新的时候进行软更新\n",
    "\tdef target_soft_update (self,net,target_net,soft_tau):\n",
    "\t\t''' soft update the target net with Polyak averaging '''\n",
    "\t\tfor target_param,param in zip (target_net.trainable_weights,net.trainable_weights):\n",
    "\t\t\ttarget_param.assign (  # copy weight value into target parameters\n",
    "\t\t\t\ttarget_param * (1.0 - soft_tau) + param * soft_tau\n",
    "\t\t\t\t# 原来参数占比 + 目前参数占比\n",
    "\t\t\t)\n",
    "\t\treturn target_net\n",
    "\n",
    "\tdef update (self,batch_size,eval_noise_scale,reward_scale = 10.,gamma = 0.9,soft_tau = 1e-3):\n",
    "\t\t''' update all networks in TD3 '''\n",
    "\t\tself.update_cnt += 1  # 计算更新次数\n",
    "\t\tstate,action,reward,next_state = self.replay_buffer.sample (batch_size)  # 从buffer sample数据\n",
    "\n",
    "\t\treward = reward[:,np.newaxis]  # expand dim， 调整形状，方便输入网络\n",
    "\n",
    "\t\t# 输入s',从target_policy_net计算a'。注意这里有加noisy的\n",
    "\t\t##改：此处为不加噪音，后续可与加入噪音进行对比\n",
    "\t\tnew_next_action = self.target_policy_net.predict (next_state)  # clipped normal noise#到底裁不裁？？？？？？？？？？？？？？？？？？？\n",
    "\n",
    "\t\t# 归一化reward.(有正有负)\n",
    "\t\t# reward = reward_scale * (reward - np.mean(reward, axis=0)) / (np.std(reward, axis=0) + 1e-6)  # normalize with batch mean and std; plus a small number to prevent numerical problem\n",
    "\n",
    "\t\t# Training Q Function\n",
    "\t\t# 把s'和a'堆叠在一起，一起输入到target_q_net。\n",
    "\t\t# 有两个qnet，我们取最小值\n",
    "\n",
    "\t\ttarget_q_input = tf.concat ([next_state,new_next_action],1)  # the dim 0 is number of samples\n",
    "\t\ttarget_q_min = tf.minimum (self.target_q_net1.predict (target_q_input),\n",
    "\t\t                           self.target_q_net2.predict (target_q_input))\n",
    "\n",
    "\t\t# 计算target_q的值，用于更新q_net\n",
    "\t\t# 之前有把done从布尔变量改为int，就是为了这里能够直接计算。\n",
    "\t\ttarget_q_value = reward + gamma * target_q_min  # if done==1, only reward\n",
    "\t\tstate = state.astype ('float32')\n",
    "\t\taction = action.astype ('float32')\n",
    "\t\taction = action.reshape (64,1)\n",
    "\t\tq_input = tf.concat ([state,action],1)  # input of q_net\n",
    "\n",
    "\t\tq_input = tf.dtypes.cast (q_input,tf.float32)\n",
    "\t\t# 更新q_net1\n",
    "\t\t# 这里其实和DQN是一样的\n",
    "\t\twith tf.GradientTape () as q1_tape:\n",
    "\t\t\tpredicted_q_value1 = self.q_net1 (q_input)\n",
    "\t\t\tq_value_loss1 = tf.reduce_mean (tf.square (predicted_q_value1 - target_q_value))\n",
    "\t\tq1_grad = q1_tape.gradient (q_value_loss1,self.q_net1.trainable_weights)\n",
    "\t\tself.q_optimizer1.apply_gradients (zip (q1_grad,self.q_net1.trainable_weights))\n",
    "\n",
    "\t\t# 更新q_net2\n",
    "\t\twith tf.GradientTape () as q2_tape:\n",
    "\t\t\tpredicted_q_value2 = self.q_net2 (q_input)\n",
    "\t\t\tq_value_loss2 = tf.reduce_mean (tf.square (predicted_q_value2 - target_q_value))\n",
    "\t\tq2_grad = q2_tape.gradient (q_value_loss2,self.q_net2.trainable_weights)\n",
    "\t\tself.q_optimizer2.apply_gradients (zip (q2_grad,self.q_net2.trainable_weights))\n",
    "\n",
    "\t\t# Training Policy Function\n",
    "\t\t# policy不是经常update的，而qnet更新一定次数，才update一次\n",
    "\t\tpolicy_loss_rec = False\n",
    "\t\tif self.update_cnt % self.policy_target_update_interval == 0:\n",
    "\t\t\t# 更新policy_net\n",
    "\t\t\twith tf.GradientTape () as p_tape:\n",
    "\t\t\t\t# 计算 action = Policy(s)，注意这里是没有noise的\n",
    "\t\t\t\tnew_action = self.policy_net (\n",
    "\t\t\t\t\tstate\n",
    "\t\t\t\t)  # no noise, deterministic policy gradients\n",
    "\n",
    "\t\t\t\t# 叠加state和action\n",
    "\t\t\t\tnew_q_input = tf.concat ([state,new_action],1)\n",
    "\t\t\t\t# ''' implementation 1 '''\n",
    "\t\t\t\t# predicted_new_q_value = tf.minimum(self.q_net1(new_q_input),self.q_net2(new_q_input))\n",
    "\t\t\t\t''' implementation 2 '''\n",
    "\t\t\t\tpredicted_new_q_value = self.q_net1 (new_q_input)\n",
    "\t\t\t\tpolicy_loss = -tf.reduce_mean (predicted_new_q_value)  # 梯度上升\n",
    "\t\t\t\tpolicy_loss_rec = policy_loss\n",
    "\t\t\tp_grad = p_tape.gradient (policy_loss,self.policy_net.trainable_weights)\n",
    "\t\t\tself.policy_optimizer.apply_gradients (zip (p_grad,self.policy_net.trainable_weights))\n",
    "\t\t\t# print(self.policy_net.)\n",
    "\n",
    "\t\t\t# Soft update the target nets\n",
    "\t\t\t# 软更新target_network三个\n",
    "\t\t\tself.target_q_net1 = self.target_soft_update (self.q_net1,self.target_q_net1,soft_tau)\n",
    "\t\t\tself.target_q_net2 = self.target_soft_update (self.q_net2,self.target_q_net2,soft_tau)\n",
    "\t\t\tself.target_policy_net = self.target_soft_update (self.policy_net,self.target_policy_net,soft_tau)\n",
    "\n",
    "\t\tif policy_loss_rec:\n",
    "\t\t\treturn q_value_loss1,q_value_loss2,policy_loss_rec\n",
    "\t\telse:\n",
    "\t\t\treturn q_value_loss1,q_value_loss2,10086\n",
    "\n",
    "\tdef save_weights (self,model_path,describe = ''):  # save trained weights\n",
    "\t\tsave_path = os.path.join (model_path,describe)\n",
    "\t\tif not os.path.exists (save_path):\n",
    "\t\t\tos.makedirs (save_path)\n",
    "\t\tself.q_net1.save (save_path + '/model_q_net1.hdf5')\n",
    "\t\tself.q_net2.save (save_path + '/model_q_net2.hdf5')\n",
    "\t\tself.target_q_net1.save (save_path + '/model_target_q_net1.hdf5')\n",
    "\t\tself.target_q_net2.save (save_path + '/model_target_q_net2.hdf5')\n",
    "\t\tself.policy_net.save (save_path + '/model_policy_net.hdf5')\n",
    "\t\tself.target_policy_net.save (save_path + '/model_target_policy_net.hdf5')\n",
    "\t\treturn save_path\n",
    "\n",
    "\t# tl.files.save_npz(self.q_net1.trainable_weights, name='model_q_net1.npz')\n",
    "\t# tl.files.save_npz(self.q_net2.trainable_weights, name='model_q_net2.npz')\n",
    "\t# tl.files.save_npz(self.target_q_net1.trainable_weights, name='model_target_q_net1.npz')\n",
    "\t# tl.files.save_npz(self.target_q_net2.trainable_weights, name='model_target_q_net2.npz')\n",
    "\t# tl.files.save_npz(self.policy_net.trainable_weights, name='model_policy_net.npz')\n",
    "\t# tl.files.save_npz(self.target_policy_net.trainable_weights, name='model_target_policy_net.npz')\n",
    "\n",
    "\tdef load_weights (self,save_path):  # load trained weights\n",
    "\t\tself.q_net1.load_weights (save_path + '/model_q_net1.hdf5')\n",
    "\t\tself.q_net2.load_weights (save_path + '/model_q_net2.hdf5')\n",
    "\t\tself.target_q_net1.load_weights (save_path + '/model_target_q_net1.hdf5')\n",
    "\t\tself.target_q_net2.load_weights (save_path + '/model_target_q_net2.hdf5')\n",
    "\t\tself.policy_net.load_weights (save_path + '/model_policy_net.hdf5')\n",
    "\t\tself.target_policy_net.load_weights (save_path + '/model_target_policy_net.hdf5')\n",
    "\n",
    "\n",
    "# tl.files.load_and_assign_npz(name='model_q_net1.npz', network=self.q_net1)\n",
    "# tl.files.load_and_assign_npz(name='model_q_net2.npz', network=self.q_net2)\n",
    "# tl.files.load_and_assign_npz(name='model_target_q_net1.npz', network=self.target_q_net1)\n",
    "# tl.files.load_and_assign_npz(name='model_target_q_net2.npz', network=self.target_q_net2)\n",
    "# tl.files.load_and_assign_npz(name='model_policy_net.npz', network=self.policy_net)\n",
    "# tl.files.load_and_assign_npz(name='model_target_policy_net.npz', network=self.target_policy_net)\n",
    "\n",
    "# 初始化环境\n",
    "action_max ,action_min=6.5,-1.11\n",
    "model_name = get_str_time ()\n",
    "env = SGTR_env (model=model,set_point=-56,train_datasets=train_data,mean=men_std[0],std=men_std[1])  # 环境\n",
    "action_dim = env.action_space.shape[0]  # 动作空间\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "# 初始化缓冲区\n",
    "replay_buffer = ReplayBuffer (replay_buffer_size)\n",
    "\n",
    "# 初始化agent\n",
    "td3_trainer = TD3_Trainer (replay_buffer,hidden_dim=hidden_dim,\n",
    "                           policy_target_update_interval=policy_target_update_interval,action_range=action_range,\n",
    "                           q_lr=q_lr,policy_lr=policy_lr)\n",
    "rewards=[]\n",
    "\n",
    "\n",
    "def fill_replay_buff():\n",
    "\tglobal replay_buffer\n",
    "\twith open (\n",
    "\t\t\tr'M:\\work\\project_program\\bishe\\pyproject\\RL_pid_control_critic_actor\\强化学习+Lstm\\replay_buff_\\numpy_binary.npy',\n",
    "\t\t\t'rb') as f:\n",
    "\t\tbuff_data = np.load (f,allow_pickle=True)\n",
    "\tbuff_datas = buff_data.tolist ()\n",
    "\tprint('Filling Replay Buffer.......')\n",
    "\tfor data in tqdm(buff_datas):\n",
    "\t\tstate,action,reward,next_state = data\n",
    "\t\treplay_buffer.push(state,action,reward,next_state)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train (new:bool,thread_name:str):\n",
    "\tglobal model_name,env,action_dim,state_dim,replay_buffer,td3_trainer,rewards\n",
    "\tprint(\"initializing.......\")\n",
    "\tif not new:\n",
    "\t\tfill_replay_buff ()\n",
    "\t\t# 接着上一次训练\n",
    "\t\ttd3_trainer.load_weights (\n",
    "\t\t\tr'M:\\work\\project_program\\bishe\\pyproject\\RL_pid_control_critic_actor\\强化学习+Lstm\\rl_model\\last_model\\_2022_7_23_21_3_4_96_0')\n",
    "\t\t# 获得上一次的奖励\n",
    "\t\treward_data = pd.read_csv (r\"M:\\work\\project_program\\bishe\\pyproject\\RL_pid_control_critic_actor\\强化学习+Lstm\\rewards_recoard\\_2022_7_23_21_3_4_rewards.csv\")\n",
    "\t\treward = reward_data[\"rewards\"].values.tolist ()\n",
    "\telse:\n",
    "\t\treward=[]\n",
    "\n",
    "\tframe_idx = 0  # 总步数\n",
    "\trewards = reward  # 记录每个EP的总reward\n",
    "\tmax_reward = 0  # 记录最大reward\n",
    "\t# time_spend = []\n",
    "\t# t0 = time.time ()\n",
    "\t# t_start = time.time ()\n",
    "\ttry:\n",
    "\t\twhile frame_idx < max_frames:\n",
    "\t\t\t# 小于最大步数，就继续训练\n",
    "\t\t\t# time_temp = {\"frame_idx\": frame_idx}\n",
    "\n",
    "\t\t\tstate = env.reset ()[-1]  # 初始化state\n",
    "\t\t\tstate = state.astype (np.float32)  # 整理state的类型\n",
    "\t\t\tepisode_reward = 0\n",
    "\t\t\t# time_temp[\"初始化state\"] = time.time () - t0\n",
    "\n",
    "\t\t\tstep_count = 0\n",
    "\t\t\t# 开始训练\n",
    "\n",
    "\t\t\tfor step in range (max_steps):\n",
    "\t\t\t\tt0 = time.time ()\n",
    "\t\t\t\tif frame_idx > explore_frams:  # 如果小于500步，就随机，如果大于就用get-action\n",
    "\t\t\t\t\taction = td3_trainer.policy_net.predict (\n",
    "\t\t\t\t\t\tnp.array ([state])) + tf.clip_by_value (tf.random.normal ([1],0,0.006),-1,1)\n",
    "\t\t\t\t\taction = action.numpy().tolist()[0][0]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t#指数分布随机\n",
    "\t\t\t\t\t# action = tf.clip_by_value (random.expovariate (1 / 0.878413) - 0.878413,action_min,\n",
    "\t\t\t\t\t#                            action_max)  # tf.random.uniform([1], action_min, action_max)+tf.clip_by_value(tf.random.normal([1],0,0.006),-1,1)\n",
    "\t\t\t\t\t# action=action.numpy()\n",
    "\t\t\t\t\t# 平均分布随机\n",
    "\t\t\t\t\taaa = tf.random.uniform ([1],-1,1)\n",
    "\t\t\t\t\taction=(action_max-action_min) * aaa.numpy().tolist()[0]\n",
    "\t\t\t\t#time_temp[\"选择action\"] = time.time () - t0\n",
    "\n",
    "\t\t\t\t# 与环境进行交互\n",
    "\t\t\t\t# t0 = time.time ()\n",
    "\t\t\t\tnext_state,reward,done,_ = env.step (action)\n",
    "\t\t\t\tif frame_idx > explore_frams:\n",
    "\t\t\t\t\treward += random.random () / 10\n",
    "\t\t\t\tnext_state = next_state.astype (np.float32)\n",
    "\t\t\t\taction = np.array (action).tolist ()\n",
    "\t\t\t\tdone = 1 if done == True else 0\n",
    "\t\t\t\t#time_temp[\"与环境交互\"] = time.time () - t0\n",
    "\n",
    "\t\t\t\t# 记录数据在replay_buffer\n",
    "\t\t\t\t# t0 = time.time ()\n",
    "\t\t\t\treplay_buffer.push (state,action,reward,next_state)\n",
    "\n",
    "\t\t\t\t# 赋值state，累计总reward，步数\n",
    "\t\t\t\tstate = next_state\n",
    "\t\t\t\tepisode_reward += reward\n",
    "\t\t\t\t# time_temp[\"记录数据\"] = time.time () - t0\n",
    "\n",
    "\t\t\t\tprint (f'Thread:{thread_name} | Episode: {frame_idx}/{max_frames}  | Action: {action} | Step: {step} | Step Reward: {round(reward,3)} |',end='\\r')\n",
    "\n",
    "\t\t\t\tif done:\n",
    "\t\t\t\t\tstep_count = step\n",
    "\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\t\t# 如果数据超过一个batch_size的大小，那么就开始更新\n",
    "\t\t\t\tt0 = time.time ()\n",
    "\t\t\t\tif len (replay_buffer) >= 29999 and step_count>100:\n",
    "\t\t\t\t\tfor i in range (1):  # 注意：这里更新可以更新多次！\n",
    "\t\t\t\t\t\ttd3_trainer.update (batch_size,eval_noise_scale=0.1,reward_scale=0.1)\n",
    "\t\t\t\t# time_temp[\"td3网络更新\"] = time.time () - t0\n",
    "\t\t\t\t# time_spend.append (time_temp)\n",
    "\t\t\t\tstep_count = step\n",
    "\n",
    "\n",
    "\t\t\tprint(\n",
    "\t\t\t\t'Thread:{} | Episode: {}/{}  | Episode Reward: {:.4f}  | Run Step: {} |--------Process Failed Flag-------'.format (\n",
    "\t\t\t\t\tthread_name,frame_idx,max_frames,episode_reward,step_count))\n",
    "\n",
    "\t\t\tframe_idx += 1\n",
    "\n",
    "\t\t\t# t0 = time.time ()\n",
    "\t\t\tif step_count > 100 and episode_reward > max_reward:  # 保存奖励最大的模型\n",
    "\t\t\t\tmax_reward = episode_reward\n",
    "\t\t\t\ttd3_trainer.save_weights ('rl_model/best_model',model_name+str(frame_idx)+\"_\"+str(max_reward))\n",
    "\t\t\t# print (f\"模型保存：{time.time () - t0}\\n#################\")\n",
    "\t\t\t# print(episode_reward,'###',state)\n",
    "\t\t\trewards.append (episode_reward)\n",
    "\n",
    "\texcept:\n",
    "\t\trewards_rec = pd.DataFrame (data={'rewards': rewards})\n",
    "\t\trewards_rec.to_csv (r\"rewards_recoard/\" + model_name + 'rewards.csv')\n",
    "\t\ttd3_trainer.save_weights ('rl_model/last_model',model_name+str(frame_idx)+\"_\"+str(max_reward))\n",
    "\t\treplay_data = np.array (replay_buffer.buffer,dtype='object')\n",
    "\t\tnp.save (\"replay_buff_/numpy_binary\",replay_data)\n",
    "\n",
    "\trewards_rec = pd.DataFrame (data={'rewards': rewards})\n",
    "\trewards_rec.to_csv (r\"rewards_recoard/\" + model_name + 'rewards.csv')\n",
    "\ttd3_trainer.save_weights ('rl_model/last_model',model_name + str (frame_idx) + \"_\" + str (max_reward))\n",
    "\treplay_data = np.array (replay_buffer.buffer,dtype='object')\n",
    "\tnp.save (\"replay_buff_/numpy_binary\",replay_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test ():\n",
    "\tglobal model_name,env,action_dim,state_dim,replay_buffer,td3_trainer\n",
    "\t# 装载模型权重\n",
    "\ttd3_trainer.load_weights (\n",
    "\t\tr'M:\\work\\project_program\\bishe\\pyproject\\RL_pid_control_critic_actor\\强化学习+Lstm\\rl_model\\last_model\\_2022_7_23_11_28_26_794_76.59812447160093')\n",
    "\tfor test_count in range(10):\n",
    "\t\tstate = env.reset ()[-1]  # 初始化state\n",
    "\t\tstate = state.astype (np.float32)  # 整理state的类型\n",
    "\t\tspeed_rec = []\n",
    "\t\ttemp_rec = []\n",
    "\t\taction_rec = []\n",
    "\t\treward_sum=[]\n",
    "\t\tepisode_reward=0\n",
    "\t\tfor step in tqdm (range (max_steps)):\n",
    "\t\t\taction = td3_trainer.policy_net.predict (\n",
    "\t\t\t\tnp.array ([state]))  # + tf.clip_by_value (tf.random.normal ([1],0,0.006),-1,1)\n",
    "\t\t\taction = action.data.tolist ()[0][0]\n",
    "\t\t\taction_rec.append (action)\n",
    "\t\t\t# 与环境进行交互\n",
    "\t\t\tnext_state,reward,done,_ = env.step (action)\n",
    "\t\t\tnext_state = next_state.astype (np.float32)\n",
    "\t\t\taction = np.array (action).tolist ()\n",
    "\t\t\tdone = 1 if done == True else 0\n",
    "\t\t\tprint(reward)\n",
    "\t\t\tepisode_reward+=reward\n",
    "\t\t\treward_sum.append(episode_reward)\n",
    "\n",
    "\t\t\tori_temp_before_last = env.cal_origin_val (1,state[1])\n",
    "\t\t\tori_temp_last = env.cal_origin_val (1,next_state[1])\n",
    "\t\t\ttemp_rec.append (ori_temp_before_last)\n",
    "\t\t\t# 赋值state，累计总reward，步数\n",
    "\t\t\tstate = next_state\n",
    "\n",
    "\t\t\ttemp_change_speed = (ori_temp_last - ori_temp_before_last) * 3600\n",
    "\t\t\tspeed_rec.append (temp_change_speed)\n",
    "\t\tplt.subplot (2,2,1)\n",
    "\t\tplt.plot (speed_rec)\n",
    "\t\tplt.title ('speed_rec')\n",
    "\t\tplt.subplot (2,2,2)\n",
    "\t\tplt.plot (temp_rec)\n",
    "\t\tplt.title ('temp_rec')\n",
    "\t\tplt.subplot (2,2,3)\n",
    "\t\tplt.plot (action_rec)\n",
    "\t\tplt.title ('action_rec')\n",
    "\t\tplt.subplot (2,2,4)\n",
    "\t\tplt.plot (reward_sum)\n",
    "\t\tplt.title ('reward_sum')\n",
    "\t\tplt.show ()\n",
    "\n",
    "\n",
    "def plot_reward():\n",
    "\tglobal env\n",
    "\ttemp = []\n",
    "\txlist = []\n",
    "\tfor i in range (int (10000 / 0.01)):\n",
    "\t\txlist.append (-5000 + 0.01 * i)\n",
    "\n",
    "\tfor i in xlist:\n",
    "\t\taaa = env.cal_reward (i)\n",
    "\t\ttemp.append (aaa)\n",
    "\tplt.plot (xlist,temp)\n",
    "\tplt.show ()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\t# def multi_thread ():\n",
    "\t# \tthreads = []\n",
    "\t# \tfor i in range(10):\n",
    "\t# \t\tthreads.append (\n",
    "\t# \t\t\tthreading.Thread (target=train,args=(True,f\"Thread{i}\"))\n",
    "\t# \t\t)\n",
    "\t#\n",
    "\t# \tfor thread in threads:\n",
    "\t# \t\tthread.start ()\n",
    "\t#\n",
    "\t# \tfor thread in threads:\n",
    "\t# \t\tthread.join ()\n",
    "\t#\n",
    "\t# try:\n",
    "\t# \tmulti_thread()\n",
    "\t# except:\n",
    "\t# \trewards_rec = pd.DataFrame (data={'rewards': rewards})\n",
    "\t# \trewards_rec.to_csv (r\"rewards_recoard/\" + model_name + 'rewards.csv')\n",
    "\t# \ttd3_trainer.save_weights ('rl_model/last_model',model_name)\n",
    "\t# \treplay_data = np.array (replay_buffer.buffer,dtype='object')\n",
    "\t# \tnp.save (\"numpy_binary\",replay_data)\n",
    "\n",
    "\t#train(True,\"Thread0\")\n",
    "\n",
    "\n",
    "\ttrain(True,\"Thread0\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] [*] Load model/ppo_actor.hdf5 SUCCESS!\n",
      "[TL] [*] Load model/ppo_actor_old.hdf5 SUCCESS!\n",
      "[TL] [*] Load model/ppo_critic.hdf5 SUCCESS!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:09<00:00, 104.23it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import math\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import tensorlayer as tl\n",
    "\n",
    "\n",
    "def get_str_time():\n",
    "    s = '_'\n",
    "    for i in time.localtime(time.time())[0:6]:\n",
    "        s += str(i) + '_'\n",
    "    return s\n",
    "\n",
    "\n",
    "model = keras.models.load_model(\n",
    "    r'M:\\work\\project_program\\bishe\\pyproject\\RL_pid_control_critic_actor\\强化学习+Lstm\\一回路温度控制\\温度模型\\模型\\best_model_epoch42.hdf5')\n",
    "#\n",
    "\n",
    "data_path = r'M:\\work\\project_program\\bishe\\pyproject\\RL_pid_control_critic_actor\\强化学习+Lstm\\一回路温度控制\\一回路温度train数据.npy'\n",
    "train_data = np.load(data_path, allow_pickle=True)\n",
    "train_data = train_data.astype('float64')\n",
    "men_std = np.load(\"一回路温度mean,std数据.npy\", allow_pickle=True)\n",
    "\n",
    "\n",
    "def get_str_time():\n",
    "    s = '_'\n",
    "    for i in time.localtime(time.time())[0:6]:\n",
    "        s += str(i) + '_'\n",
    "    return s\n",
    "\n",
    "\n",
    "class SGTR_env():\n",
    "    def __init__(self, model, set_point, train_datasets, mean, std):\n",
    "        self.action_space = np.array([0] * 1)\n",
    "        self.observation_space = np.array([0] * 12)\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.response = []\n",
    "        self.set_point = set_point\n",
    "        self.train_datasets = train_datasets\n",
    "        self.model = model\n",
    "        self.state = self.train_datasets[8000]  # [random.randint(0, train_datasets.shape[0])]\n",
    "        self.step_count = 0  # 步数计数\n",
    "\n",
    "    def reset(self):\n",
    "        self.step_count = 0\n",
    "        # begin_index = range(1800, self.train_datasets.shape[0] - 3000, 100)\n",
    "        state = self.train_datasets[8000]\n",
    "        #  state = self.train_datasets[random.randint(0, self.train_datasets.shape[0]-1)]\n",
    "        self.state = np.array(state)\n",
    "        return np.array(state).reshape(120, )\n",
    "\n",
    "    def cal_origin_val(self, pos, now_val):\n",
    "        \"\"\"\n",
    "\t\t计算未归一化的值\n",
    "\t\t\"\"\"\n",
    "        val = now_val * self.std[pos] + self.mean[pos]\n",
    "        return val\n",
    "\n",
    "    def justice_down(self, error):\n",
    "        \"\"\"\n",
    "\t\t判断是否达到失败条件，deltaT<10或70分钟内未能实现一二回路压力平衡（小于1MP）\n",
    "\t\t\"\"\"\n",
    "        # ori_deltaT = self.cal_origin_val(6, next_state[-1, 6])\n",
    "        # ori_pressure = self.cal_origin_val(0,next_state[-1, 0])\n",
    "        # if ori_deltaT < 7.9 and step > 100:  # or (step>4200 and ori_pressure<1):\n",
    "        #     return True\n",
    "        #\n",
    "        # else:\n",
    "        #     return False\n",
    "        if abs(error) > 1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def cal_reward(self, error):\n",
    "        if error > -0.01 and error < 0.01:\n",
    "            reward_1 = 2\n",
    "        else:\n",
    "            reward_1 = 0\n",
    "        reward_2 = -2*abs(error)\n",
    "        reward = reward_1 + reward_2\n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        self.step_count += 1\n",
    "        self.state[-1, -1] = action\n",
    "        # model(test_input, training=False)\n",
    "        next_variable_state = np.array(self.model(np.array([self.state]), training=False))\n",
    "        next_action = action\n",
    "        zip_state_action = np.append(next_variable_state, next_action).reshape(1, -1)\n",
    "        next_state = np.row_stack((self.state, zip_state_action))\n",
    "        next_state = np.delete(next_state, 0, axis=0)\n",
    "        # ori_temp_last = self.cal_origin_val(1, next_state[-1, 1])\n",
    "        # ori_temp_before_last = self.cal_origin_val(1, next_state[-2, 1])\n",
    "        error = self.set_point - next_state[-1, 1]\n",
    "        reward = self.cal_reward(error)\n",
    "        done = self.justice_down(error)\n",
    "        self.state = next_state\n",
    "\n",
    "        return np.array(next_state).reshape(1,120 ), reward, done, {}\n",
    "\n",
    "\n",
    "# import tensorflow_probability as tfp\n",
    "# import tensorlayer as tl\n",
    "#\n",
    "# tfd = tfp.distributions\n",
    "# Normal = tfd.Normal\n",
    "#\n",
    "# tl.logging.set_verbosity(tl.logging.DEBUG)\n",
    "\n",
    "random.seed(2)\n",
    "np.random.seed(2)\n",
    "\n",
    "#####################  hyper parameters  ####################\n",
    "RANDOMSEED = 1  # random seed\n",
    "\n",
    "EP_MAX = 1000  # total number of episodes for training\n",
    "EP_LEN = 500  # total number of steps for each episode\n",
    "GAMMA = 0.9  # reward discount\n",
    "A_LR = 0.0001  # learning rate for actor\n",
    "C_LR = 0.0002  # learning rate for critic\n",
    "BATCH = 32  # update batchsize\n",
    "A_UPDATE_STEPS = 10  # actor update steps\n",
    "C_UPDATE_STEPS = 10  # critic update steps\n",
    "S_DIM, A_DIM = 120, 1  # state dimension, action dimension\n",
    "EPS = 1e-8  # epsilon\n",
    "\n",
    "# 注意：这里是PPO1和PPO2的相关的参数。\n",
    "METHOD = [\n",
    "    dict(name='kl_pen', kl_target=0.01, lam=0.5),  # KL penalty  PPO1\n",
    "    dict(name='clip', epsilon=0.2),  # Clipped surrogate objective, find this is better  PPO2\n",
    "][1]  # choose the method for optimization\n",
    "\n",
    "\n",
    "# METHOD=METHOD[1]\n",
    "###############################  PPO  ####################################\n",
    "\n",
    "class PPO(object):\n",
    "    '''\n",
    "    PPO 类\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # 构建critic网络：\n",
    "        # 输入state，输出V值\n",
    "        tfs = tl.layers.Input([None, S_DIM], tf.float32, 'state')\n",
    "        l1 = tl.layers.Dense(100, tf.nn.relu)(tfs)\n",
    "        v = tl.layers.Dense(1)(l1)\n",
    "        self.critic = tl.models.Model(tfs, v)\n",
    "        self.critic.train()\n",
    "\n",
    "        # 构建actor网络：\n",
    "        # actor有两个actor 和 actor_old， actor_old的主要功能是记录行为策略的版本。\n",
    "        # 输入是state，输出是描述动作分布的mu和sigma\n",
    "        self.actor = self._build_anet('pi', trainable=True)\n",
    "        self.actor_old = self._build_anet('oldpi', trainable=False)\n",
    "        self.actor_opt = tf.optimizers.Adam(A_LR)\n",
    "        self.critic_opt = tf.optimizers.Adam(C_LR)\n",
    "\n",
    "    def a_train(self, tfs, tfa, tfadv):\n",
    "        '''\n",
    "        更新策略网络(policy network)\n",
    "        '''\n",
    "        # 输入时s，a，td-error。这个和AC是类似的。\n",
    "        tfs = np.array(tfs, np.float32)  # state\n",
    "        tfa = np.array(tfa, np.float32)  # action\n",
    "        tfadv = np.array(tfadv, np.float32)  # td-error\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # 【敲黑板】这里是重点！！！！\n",
    "            # 我们需要从两个不同网络，构建两个正态分布pi，oldpi。\n",
    "            mu, sigma = self.actor(tfs)\n",
    "            pi = tfp.distributions.Normal(mu, sigma)\n",
    "\n",
    "            mu_old, sigma_old = self.actor_old(tfs)\n",
    "            oldpi = tfp.distributions.Normal(mu_old, sigma_old)\n",
    "\n",
    "            # ratio = tf.exp(pi.log_prob(self.tfa) - oldpi.log_prob(self.tfa))\n",
    "            # 在新旧两个分布下，同样输出a的概率的比值\n",
    "            # 除以(oldpi.prob(tfa) + EPS)，其实就是做了import-sampling。怎么解释这里好呢\n",
    "            # 本来我们是可以直接用pi.prob(tfa)去跟新的，但为了能够更新多次，我们需要除以(oldpi.prob(tfa) + EPS)。\n",
    "            # 在AC或者PG，我们是以1,0作为更新目标，缩小动作概率到1or0的差距\n",
    "            # 而PPO可以想作是，以oldpi.prob(tfa)出发，不断远离（增大or缩小）的过程。\n",
    "            ratio = pi.prob(tfa) / (oldpi.prob(tfa) + EPS)\n",
    "            # 这个的意义和带参数更新是一样的。\n",
    "            surr = ratio * tfadv\n",
    "\n",
    "            # 我们还不能让两个分布差异太大。\n",
    "            # PPO1\n",
    "            if METHOD['name'] == 'kl_pen':\n",
    "                tflam = METHOD['lam']\n",
    "                kl = tfp.distributions.kl_divergence(oldpi, pi)\n",
    "                kl_mean = tf.reduce_mean(kl)\n",
    "                aloss = -(tf.reduce_mean(surr - tflam * kl))\n",
    "            # PPO2：\n",
    "            # 很直接，就是直接进行截断。\n",
    "            else:  # clipping method, find this is better\n",
    "                aloss = -tf.reduce_mean(\n",
    "                    tf.minimum(ratio * tfadv,  # surr\n",
    "                               tf.clip_by_value(ratio, 1. - METHOD['epsilon'], 1. + METHOD['epsilon']) * tfadv)\n",
    "                )\n",
    "        a_gard = tape.gradient(aloss, self.actor.trainable_weights)\n",
    "\n",
    "        self.actor_opt.apply_gradients(zip(a_gard, self.actor.trainable_weights))\n",
    "\n",
    "        if METHOD['name'] == 'kl_pen':\n",
    "            return kl_mean\n",
    "\n",
    "    def update_old_pi(self):\n",
    "        '''\n",
    "        更新actor_old参数。\n",
    "        '''\n",
    "        for p, oldp in zip(self.actor.trainable_weights, self.actor_old.trainable_weights):\n",
    "            oldp.assign(p)\n",
    "\n",
    "    def c_train(self, tfdc_r, s):\n",
    "        '''\n",
    "        更新Critic网络\n",
    "        '''\n",
    "        tfdc_r = np.array(tfdc_r, dtype=np.float32)  # tfdc_r可以理解为PG中就是G，通过回溯计算。只不过这PPO用TD而已。\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            v = self.critic(s)\n",
    "            advantage = tfdc_r - v  # 就是我们说的td-error\n",
    "            closs = tf.reduce_mean(tf.square(advantage))\n",
    "\n",
    "        grad = tape.gradient(closs, self.critic.trainable_weights)\n",
    "        self.critic_opt.apply_gradients(zip(grad, self.critic.trainable_weights))\n",
    "\n",
    "    def cal_adv(self, tfs, tfdc_r):\n",
    "        '''\n",
    "        计算advantage，也就是td-error\n",
    "        '''\n",
    "        tfdc_r = np.array(tfdc_r, dtype=np.float32)\n",
    "        advantage = tfdc_r - self.critic(tfs)  # advantage = r - gamma * V(s_)\n",
    "        return advantage.numpy()\n",
    "\n",
    "    def update(self, s, a, r):\n",
    "        '''\n",
    "        Update parameter with the constraint of KL divergent\n",
    "        :param s: state\n",
    "        :param a: act\n",
    "        :param r: reward\n",
    "        :return: None\n",
    "        '''\n",
    "        s, a, r = s.astype(np.float32), a.astype(np.float32), r.astype(np.float32)\n",
    "\n",
    "        self.update_old_pi()\n",
    "        adv = self.cal_adv(s, r)\n",
    "        # adv = (adv - adv.mean())/(adv.std()+1e-6)  # sometimes helpful\n",
    "\n",
    "        # update actor\n",
    "        #### PPO1比较复杂:\n",
    "        # 动态调整参数 adaptive KL penalty\n",
    "        if METHOD['name'] == 'kl_pen':\n",
    "            for _ in range(A_UPDATE_STEPS):\n",
    "                kl = self.a_train(s, a, adv)\n",
    "                if kl > 4 * METHOD['kl_target']:  # this in in google's paper\n",
    "                    break\n",
    "            if kl < METHOD['kl_target'] / 1.5:  # adaptive lambda, this is in OpenAI's paper\n",
    "                METHOD['lam'] /= 2\n",
    "            elif kl > METHOD['kl_target'] * 1.5:\n",
    "                METHOD['lam'] *= 2\n",
    "            METHOD['lam'] = np.clip(\n",
    "                METHOD['lam'], 1e-4, 10\n",
    "            )  # sometimes explode, this clipping is MorvanZhou's solution\n",
    "\n",
    "        #### PPO2比较简单，直接就进行a_train更新:\n",
    "        # clipping method, find this is better (OpenAI's paper)\n",
    "        else:\n",
    "            for _ in range(A_UPDATE_STEPS):\n",
    "                self.a_train(s, a, adv)\n",
    "\n",
    "        # 更新 critic\n",
    "        for _ in range(C_UPDATE_STEPS):\n",
    "            self.c_train(r, s)\n",
    "\n",
    "    def _build_anet(self, name, trainable):\n",
    "        '''\n",
    "        Build policy network\n",
    "        :param name: name\n",
    "        :param trainable: trainable flag\n",
    "        :return: policy network\n",
    "        '''\n",
    "        # 连续动作型问题，输出mu和sigma。\n",
    "        tfs = tl.layers.Input([None, S_DIM], tf.float32, name + '_state')\n",
    "        l1 = tl.layers.Dense(100, tf.nn.relu, name=name + '_l1')(tfs)\n",
    "\n",
    "        a = tl.layers.Dense(A_DIM, tf.nn.tanh, name=name + '_a')(l1)\n",
    "        mu = tl.layers.Lambda(lambda x: x * 2, name=name + '_lambda')(a)\n",
    "\n",
    "        sigma = tl.layers.Dense(A_DIM, tf.nn.softplus, name=name + '_sigma')(l1)\n",
    "\n",
    "        model = tl.models.Model(tfs, [mu, sigma], name)\n",
    "\n",
    "        if trainable:\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "        return model\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        '''\n",
    "        Choose action\n",
    "        :param s: state\n",
    "        :return: clipped act\n",
    "        '''\n",
    "        s = s[np.newaxis, :].astype(np.float32)\n",
    "        mu, sigma = self.actor(s)  # 通过actor计算出分布的mu和sigma\n",
    "        pi = tfp.distributions.Normal(mu, sigma)  # 用mu和sigma构建正态分布\n",
    "        a = tf.squeeze(pi.sample(1), axis=0)[0]  # 根据概率分布随机出动作\n",
    "        return np.clip(a, -2, 2)  # 最后sample动作，并进行裁剪。\n",
    "\n",
    "    def get_v(self, s):\n",
    "        '''\n",
    "        计算value值。\n",
    "        '''\n",
    "        s = s.astype(np.float32)\n",
    "        if s.ndim < 2: s = s[np.newaxis, :]  # 要和输入的形状对应。\n",
    "        return self.critic(s)[0, 0]\n",
    "\n",
    "    def save_ckpt(self):\n",
    "        \"\"\"\n",
    "        save trained weights\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        if not os.path.exists('model'):\n",
    "            os.makedirs('model')\n",
    "        tl.files.save_weights_to_hdf5('model/ppo_actor.hdf5', self.actor)\n",
    "        tl.files.save_weights_to_hdf5('model/ppo_actor_old.hdf5', self.actor_old)\n",
    "        tl.files.save_weights_to_hdf5('model/ppo_critic.hdf5', self.critic)\n",
    "\n",
    "    def load_ckpt(self):\n",
    "        \"\"\"\n",
    "        load trained weights\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        tl.files.load_hdf5_to_weights_in_order('model/ppo_actor.hdf5', self.actor)\n",
    "        tl.files.load_hdf5_to_weights_in_order('model/ppo_actor_old.hdf5', self.actor_old)\n",
    "        tl.files.load_hdf5_to_weights_in_order('model/ppo_critic.hdf5', self.critic)\n",
    "\n",
    "\n",
    "def train():\n",
    "    env = SGTR_env(model=model, set_point=0.0739, train_datasets=train_data, mean=men_std[0], std=men_std[1])\n",
    "\n",
    "    # reproducible\n",
    "    np.random.seed(RANDOMSEED)\n",
    "    tf.random.set_seed(RANDOMSEED)\n",
    "\n",
    "    ppo = PPO()\n",
    "\n",
    "    all_ep_r = []\n",
    "\n",
    "    # 更新流程：\n",
    "    for ep in range(EP_MAX):\n",
    "        s = env.reset()\n",
    "        s=s.reshape(1,120)\n",
    "        buffer_s, buffer_a, buffer_r = [], [], []\n",
    "        ep_r = 0\n",
    "        t0 = time.time()\n",
    "        for t in range(EP_LEN):  # in one episode\n",
    "            # env.render()\n",
    "            a = ppo.choose_action(s)\n",
    "            s_, r, done, _ = env.step(a)\n",
    "            buffer_s.append(s)\n",
    "            buffer_a.append(a)\n",
    "            buffer_r.append(r)  # 对奖励进行归一化。有时候会挺有用的。所以我们说说，奖励是个主观的东西。\n",
    "            s = s_\n",
    "            ep_r += r\n",
    "\n",
    "            # # N步更新的方法，每BATCH步了就可以进行一次更新\n",
    "            if (t + 1) % BATCH == 0 or t == EP_LEN - 1:\n",
    "                v_s_ = ppo.get_v(s_)  # 计算n步中最后一个state的v_s_\n",
    "\n",
    "                # 和PG一样，向后回溯计算。\n",
    "                discounted_r = []\n",
    "                for r in buffer_r[::-1]:\n",
    "                    v_s_ = r + GAMMA * v_s_\n",
    "                    discounted_r.append(v_s_)\n",
    "                discounted_r.reverse()\n",
    "\n",
    "                # 所以这里的br并不是每个状态的reward，而是通过回溯计算的V值\n",
    "                bs, ba, br = np.vstack(buffer_s), np.vstack(buffer_a), np.array(discounted_r)[:, np.newaxis]\n",
    "                buffer_s, buffer_a, buffer_r = [], [], []\n",
    "                ppo.update(bs, ba, br)\n",
    "\n",
    "        if ep == 0:\n",
    "            all_ep_r.append(ep_r)\n",
    "        else:\n",
    "            all_ep_r.append(all_ep_r[-1] * 0.9 + ep_r * 0.1)\n",
    "        print(\n",
    "            'Episode: {}/{}  | Episode Reward: {:.4f}  | Running Time: {:.4f}'.format(\n",
    "                ep, EP_MAX, ep_r,\n",
    "                time.time() - t0\n",
    "            )\n",
    "        )\n",
    "\n",
    "    ppo.save_ckpt()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "\tfrom tqdm import tqdm\n",
    "\tenv = SGTR_env(model=model, set_point=0.0739, train_datasets=train_data, mean=men_std[0], std=men_std[1])\n",
    "\n",
    "\n",
    "\t# reproducible\n",
    "\tnp.random.seed(RANDOMSEED)\n",
    "\ttf.random.set_seed(RANDOMSEED)\n",
    "\n",
    "\t#ppo = PPO()\n",
    "\n",
    "\tstep_r = []\n",
    "\tsum_r=[0]\n",
    "\n",
    "\tppo.load_ckpt()\n",
    "\ts = env.reset().reshape(1,120)\n",
    "\tstate=[s]\n",
    "\tfor i in tqdm(range(1000)):\n",
    "\t\t# if i ==200:\n",
    "\t\t#     env.set_point=[-0.36,-0.23]\n",
    "\t\ts_, r, done, _ = env.step(ppo.choose_action(s))\n",
    "\t\tstep_r.append(r)\n",
    "\t\tsum_r.append(r+sum_r[-1])\n",
    "\t\tstate.append(s_)\n",
    "\t\ts=s_\n",
    "\t\tif done:\n",
    "\t\t\tbreak"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "state=np.array(state)\n",
    "#aaa,bbb,ccc,ddd=state[:,:,[4]],state[:,:,[5]],state[:,:,[6]],state[:,:,[7]]\n",
    "eee,fff,ggg,hhh=state[:,:,[0]],state[:,:,[1]],state[:,:,[2]],state[:,:,[3]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fff.reshape(1,1001).T)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.04429093],\n       [ 0.04429093],\n       [ 0.04429093],\n       ...,\n       [-0.4300786 ],\n       [-0.43009278],\n       [-0.43007106]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(fff.reshape(1,1001).T)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}