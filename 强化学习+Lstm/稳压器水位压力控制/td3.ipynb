{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_str_time():\n",
    "    s = '_'\n",
    "    for i in time.localtime(time.time())[0:6]:\n",
    "        s += str(i) + '_'\n",
    "    return s\n",
    "\n",
    "\n",
    "class SGTR_env():\n",
    "    def __init__(self, model, set_point, train_datasets, mean, std):\n",
    "        self.action_space = np.array([0] * 4)\n",
    "        self.observation_space = np.array([0] * 8)\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.response = []\n",
    "        self.set_point = set_point\n",
    "        self.train_datasets = train_datasets\n",
    "        self.model = model\n",
    "        self.state = self.train_datasets[random.randint(0, train_datasets.shape[0])]\n",
    "        self.step_count = 0  # 步数计数\n",
    "\n",
    "    def reset(self):\n",
    "        self.step_count = 0\n",
    "        # begin_index = [2500,6500,11000,14500]\n",
    "        begin_index = range(1800, self.train_datasets.shape[0] - 3000, 100)\n",
    "        state = self.train_datasets[random.sample(begin_index, 1)[0]]  # 10420]\n",
    "        #  state = self.train_datasets[random.randint(0, self.train_datasets.shape[0]-1)]\n",
    "        self.state = np.array(state)\n",
    "        return np.array(state)\n",
    "\n",
    "    def cal_origin_val(self, pos, now_val):\n",
    "        \"\"\"\n",
    "        计算未归一化的值\n",
    "        \"\"\"\n",
    "        val = now_val * self.std[pos] + self.mean[pos]\n",
    "        return val\n",
    "\n",
    "    def justice_down(self, next_state, step):\n",
    "        \"\"\"\n",
    "        判断是否达到失败条件，deltaT<10或70分钟内未能实现一二回路压力平衡（小于1MP）\n",
    "        \"\"\"\n",
    "        ori_deltaT = self.cal_origin_val(6, next_state[-1, 6])\n",
    "        # ori_pressure = self.cal_origin_val(0,next_state[-1, 0])\n",
    "        if ori_deltaT < 10:  # or (step>4200 and ori_pressure<1):\n",
    "            return True\n",
    "\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def cal_reward(self, speed):\n",
    "        # if speed < 29.276 and speed > -141.276:\n",
    "        # \treward = -0.001 * speed ** 2 - 0.112 * speed + 4.136\n",
    "        # elif speed < -141.276:\n",
    "        # \treward = 0.01 * speed + 1.2\n",
    "        # else:\n",
    "        # \treward = -0.2 * speed + 6\n",
    "        if speed <= -56:\n",
    "            mu, sigma = -56, 5\n",
    "            reward = (1 / (math.sqrt(2 * math.pi) * sigma) * math.e ** (\n",
    "                    -(speed - mu) ** 2 / (2 * sigma ** 2))) * 37 - 0.5\n",
    "\n",
    "        elif -10 >= speed > -56:\n",
    "            reward = (-1 / 23) * speed + (1 / 2 - 10 / 23)\n",
    "        elif speed > -10:\n",
    "            reward = (-1 / 20) * speed  # +(19/42)\n",
    "        else:\n",
    "            print(\"计算奖励错误\")\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        self.step_count += 1\n",
    "        self.state[-1, -1] = action\n",
    "        # model(test_input, training=False)\n",
    "        next_variable_state = np.array(self.model(np.array([self.state]), training=False))\n",
    "        next_action = action\n",
    "        zip_state_action = np.append(next_variable_state, next_action).reshape(1, -1)\n",
    "        next_state = np.row_stack((self.state, zip_state_action))\n",
    "        next_state = np.delete(next_state, 0, axis=0)\n",
    "        ori_temp_last = self.cal_origin_val(1, next_state[-1, 1])\n",
    "        ori_temp_before_last = self.cal_origin_val(1, next_state[-2, 1])\n",
    "        temp_change_speed = (ori_temp_last - ori_temp_before_last) * 3600\n",
    "        reward = self.cal_reward(temp_change_speed)\n",
    "        done = self.justice_down(next_state, self.step_count)\n",
    "        self.state = next_state\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "\n",
    "# import tensorflow_probability as tfp\n",
    "# import tensorlayer as tl\n",
    "#\n",
    "# tfd = tfp.distributions\n",
    "# Normal = tfd.Normal\n",
    "#\n",
    "# tl.logging.set_verbosity(tl.logging.DEBUG)\n",
    "\n",
    "# random.seed (2)\n",
    "# np.random.seed (2)\n",
    "# tf.random.set_seed (2)  # reproducible\n",
    "\n",
    "#####################  hyper parameters  ####################\n",
    "# choose env\n",
    "ENV = 'Pendulum-v0'\n",
    "action_range = 1.  # scale action, [-action_range, action_range]\n",
    "\n",
    "# RL training\n",
    "max_frames = 4e6  # total number of steps for training\n",
    "test_frames = 300  # total number of steps for testing\n",
    "max_steps = 2000  # maximum number of steps for one episode\n",
    "batch_size = 64  # udpate batchsize\n",
    "explore_frams = 100  # 500 for random action sampling in the beginning of training\n",
    "update_itr = 3  # repeated updates for single step\n",
    "hidden_dim = 32  # size of hidden layers for networks\n",
    "q_lr = 3e-4  # q_net learning rate\n",
    "policy_lr = 3e-4  # policy_net learning rate\n",
    "policy_target_update_interval = 3  # delayed steps for updating the policy network and target networks\n",
    "explore_noise_scale = 1.0  # range of action noise for exploration\n",
    "eval_noise_scale = 0.5  # range of action noise for evaluation of action value\n",
    "reward_scale = 1.  # value range of reward\n",
    "replay_buffer_size = 30000  # size of replay buffer\n",
    "\n",
    "\n",
    "###############################  TD3  ####################################\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    '''\n",
    "    a ring buffer for storing transitions and sampling for training\n",
    "    :state: (state_dim,)\n",
    "    :action: (action_dim,)\n",
    "    :reward: (,), scalar\n",
    "    :next_state: (state_dim,)\n",
    "    :done: (,), scalar (0 and 1) or bool (True and False)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity  # buffer的最大值\n",
    "        self.buffer = []  # buffer列表\n",
    "        self.position = 0  # 当前输入的位置，相当于指针\n",
    "\n",
    "    def push(self, state, action, reward, next_state):\n",
    "        # 如果buffer的长度小于最大值，也就是说，第一环的时候，需要先初始化一个“空间”，这个空间值为None，再给这个空间赋值。\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state)\n",
    "        self.position = int((self.position + 1) % self.capacity)  # as a ring buffer\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state = map(np.stack, zip(*batch))  # stack for each element\n",
    "        '''\n",
    "        the * serves as unpack: sum(a,b) <=> batch=(a,b), sum(*batch) ;\n",
    "        zip: a=[1,2], b=[2,3], zip(a,b) => [(1, 2), (2, 3)] ;\n",
    "        the map serves as mapping the function on each list element: map(square, [2,3]) => [4,9] ;\n",
    "        np.stack((1,2)) => array([1, 2])\n",
    "        '''\n",
    "        return state, action, reward, next_state\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# 在代码中没有用到，但我们可以学习下，这里是直接修改gym环境的动作输出，把输出归一化。\n",
    "# class NormalizedActions(gym.ActionWrapper):\n",
    "# \t''' normalize the actions to be in reasonable range '''\n",
    "#\n",
    "# \tdef _action(self, action):\n",
    "# \t\tlow = self.action_space.low  #动作空间最小值\n",
    "# \t\thigh = self.action_space.high  #动作空间最大值\n",
    "#\n",
    "# \t\taction = low + (action + 1.0) * 0.5 * (high - low)\n",
    "# \t\taction = np.clip(action, low, high)\n",
    "#\n",
    "# \t\treturn action\n",
    "#\n",
    "# \tdef _reverse_action(self, action):\n",
    "# \t\tlow = self.action_space.low\n",
    "# \t\thigh = self.action_space.high\n",
    "#\n",
    "# \t\taction = 2 * (action - low) / (high - low) - 1\n",
    "# \t\taction = np.clip(action, low, high)\n",
    "#\n",
    "# \t\treturn action\n",
    "\n",
    "\n",
    "class QNetwork():\n",
    "    ''' the network for evaluate values of state-action pairs: Q(s,a) '''\n",
    "\n",
    "    def __init__(self, num_inputs, num_actions, hidden_dim, init_w=3e-3):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.input_dim = num_inputs + num_actions\n",
    "        self.net = self.get_net()\n",
    "\n",
    "    # w_init = tf.keras.initializers.glorot_normal(seed=None)\n",
    "    # w_init = tf.random_uniform_initializer(-init_w, init_w)\n",
    "    #\n",
    "    # self.linear1 = Dense(n_units=hidden_dim, act=tf.nn.relu, W_init=w_init, in_channels=input_dim, name='q1')\n",
    "    # self.linear2 = Dense(n_units=hidden_dim, act=tf.nn.relu, W_init=w_init, in_channels=hidden_dim, name='q2')\n",
    "    # self.linear3 = Dense(n_units=1, W_init=w_init, in_channels=hidden_dim, name='q3')\n",
    "\n",
    "    def get_net(self, name='Q_model'):\n",
    "        inputs = keras.layers.Input(shape=[self.input_dim], name='C_input')\n",
    "        x = keras.layers.Dense(64, activation='tanh', name='C_l1')(inputs)\n",
    "        # x = keras.layers.BatchNormalization()(x)\n",
    "        x = keras.layers.Dense(32, activation='tanh', name='C_l2')(x)\n",
    "        # x = keras.layers.BatchNormalization()(x)\n",
    "        x = keras.layers.Dense(1, name='C_out')(x)\n",
    "        net = keras.Model(inputs=[inputs], outputs=[x], name='Critic' + name)\n",
    "        return net\n",
    "\n",
    "\n",
    "# def forward(self, input):\n",
    "#     x = self.tanh1(input)\n",
    "#     x = self.tanh2(x)\n",
    "#     x = self.tanh3(x)\n",
    "#     return x\n",
    "\n",
    "\n",
    "class PolicyNetwork():\n",
    "    ''' the network for generating non-determinstic (Gaussian distributed) action from the state input '''\n",
    "\n",
    "    def __init__(self, num_inputs, num_actions, hidden_dim, action_range=1., init_w=3e-3):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        get_custom_objects().update({'swish': Activation(self.self_act)})\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.action_range = action_range\n",
    "        self.num_actions = num_actions\n",
    "        self.net = self.get_net()\n",
    "\n",
    "    # w_init = tf.keras.initializers.glorot_normal(seed=None)\n",
    "    # w_init = tf.random_uniform_initializer(-init_w, init_w)\n",
    "\n",
    "    # self.tanh1 = keras.layers.Dense(hidden_dim, activation='rtanh',\n",
    "    #                                   name='policy1')  #Dense(n_units=hidden_dim, act=tf.nn.rtanh, W_init=w_init, in_channels=num_inputs, name='policy1')\n",
    "    # self.tanh2 = keras.layers.Dense(hidden_dim, activation='rtanh',\n",
    "    #                                   name='policy2')  #Dense(n_units=hidden_dim, act=tf.nn.rtanh, W_init=w_init, in_channels=hidden_dim, name='policy2')\n",
    "    # self.tanh3 = keras.layers.Dense(hidden_dim, activation='rtanh',\n",
    "    #                                   name='policy3')  #Dense(n_units=hidden_dim, act=tf.nn.rtanh, W_init=w_init, in_channels=hidden_dim, name='policy3')\n",
    "    #\n",
    "    # self.output_tanh = keras.layers.Dense(num_actions,activation='tanh',\n",
    "    #                                         name='policy_output')  #Dense(n_units=num_actions, W_init=w_init, b_init=tf.random_uniform_initializer(-init_w, init_w), in_channels=hidden_dim, name='policy_output')\n",
    "\n",
    "    def get_net(self, name='policy_model'):\n",
    "        \"\"\"\n",
    "        Build actor network\n",
    "        :param input_state_shape: state\n",
    "        :param name: name\n",
    "        :return: act\n",
    "        \"\"\"\n",
    "        inputs = keras.layers.Input(shape=[self.num_inputs], name='A_input')\n",
    "        # x = keras.layers.BatchNormalization()(inputs)\n",
    "        x = keras.layers.Dense(64, activation='tanh', name='policy1')(inputs)\n",
    "        # x = keras.layers.BatchNormalization()(x)\n",
    "        x = keras.layers.Dense(32, activation='tanh', name='policy2')(x)\n",
    "        # x = keras.layers.BatchNormalization()(x)\n",
    "        x = keras.layers.Dense(1, activation='sigmoid', name='policy3')(x)\n",
    "        # x = keras.layers.Dense(self.num_actions, activation='swish', name='policy_output')(x)\n",
    "        x = keras.layers.Lambda(lambda x: (np.array(action_max - action_min) * (x) + action_min))(\n",
    "            x)  # 注意这里，先用tanh把范围限定在[-1,1]之间，再进行映射\n",
    "\n",
    "        net = keras.Model(inputs=[inputs], outputs=[x], name='Actor' + name)\n",
    "        return net\n",
    "\n",
    "    def self_act(self, x):\n",
    "        # print(x)\n",
    "        return tf.clip_by_value(x, action_min, action_max)  # 1/(1+tf.math.exp(-x))\n",
    "\n",
    "\n",
    "# def forward(self, state):\n",
    "#     #x = self.linear0(state)\n",
    "#     x = self.linear1(state)\n",
    "#     x = self.linear2(x)\n",
    "#     x = self.linear3(x)\n",
    "#\n",
    "#     output = self.output_linear(x)  # unit range output [-1, 1]\n",
    "#\n",
    "#     return output\n",
    "\n",
    "# def evaluate(self, state, eval_noise_scale):\n",
    "#     '''\n",
    "#     generate action with state for calculating gradients;\n",
    "#     eval_noise_scale: as the trick of target policy smoothing, for generating noisy actions.\n",
    "#     '''\n",
    "#     state = state.astype(np.float32)        #状态的type整理\n",
    "#     action = self.forward(state)            #通过state计算action，注意这里action范围是[-1,1]\n",
    "#\n",
    "#     #action = self.action_range * action     #映射到游戏的action取值范围\n",
    "#\n",
    "#     # add noise\n",
    "#     # normal = Normal(0, 1)                   #建立一个正态分布\n",
    "#     # eval_noise_clip = 2 * eval_noise_scale  #对噪声进行上下限裁剪。eval_noise_scale\n",
    "#     # noise = normal.sample(action.shape) * eval_noise_scale      #弄个一个noisy和action的shape一致，然后乘以scale\n",
    "#     # noise = tf.clip_by_value(noise, -eval_noise_clip, eval_noise_clip)  #对noisy进行剪切，不要太大也不要太小\n",
    "#     # action = action + noise                 #action加上噪音\n",
    "#\n",
    "#     return action\n",
    "\n",
    "# 输入state，输出action\n",
    "def get_action(self, state, explore_noise_scale):\n",
    "    ''' generate action with state for interaction with envronment '''\n",
    "    action = self.net.predict(state)  # 这里的forward函数，就是输入state，然后通过state输出action。只不过形式不一样而已。最后的激活函数式tanh，所以范围是[-1, 1]\n",
    "    # print(action,type(action))\n",
    "    action = action.numpy()[0]  # 获得的action变成矩阵。\n",
    "\n",
    "    # add noise\n",
    "    # normal = Normal(0, 1)                   #生成normal这样一个正态分布\n",
    "    # noise = normal.sample(action.shape) * explore_noise_scale       #在正态分布中抽样一个和action一样shape的数据，然后乘以scale\n",
    "    # action = self.action_range * action + noise     #action乘以动作的范围，加上noise\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "def sample_action(self):\n",
    "    ''' generate random actions for exploration '''\n",
    "    a = tf.random.uniform([self.num_actions], action_min, action_max)\n",
    "\n",
    "    return a.numpy()\n",
    "\n",
    "\n",
    "class TD3_Trainer():\n",
    "\n",
    "    def __init__(\n",
    "            self, replay_buffer, hidden_dim, action_range, policy_target_update_interval=1, q_lr=3e-4, policy_lr=3e-4\n",
    "    ):\n",
    "        self.replay_buffer = replay_buffer\n",
    "\n",
    "        # initialize all networks\n",
    "        # 用两个Qnet来估算，doubleDQN的想法。同时也有两个对应的target_q_net\n",
    "        self.q_net1 = QNetwork(state_dim, action_dim, hidden_dim).net\n",
    "        self.q_net2 = QNetwork(state_dim, action_dim, hidden_dim).net\n",
    "        self.target_q_net1 = QNetwork(state_dim, action_dim, hidden_dim).net\n",
    "        self.target_q_net2 = QNetwork(state_dim, action_dim, hidden_dim).net\n",
    "        self.policy_net = PolicyNetwork(state_dim, action_dim, hidden_dim, action_range).net\n",
    "        self.target_policy_net = PolicyNetwork(state_dim, action_dim, hidden_dim, action_range).net\n",
    "        print('Q Network (1,2): ', self.q_net1)\n",
    "        print('Policy Network: ', self.policy_net)\n",
    "\n",
    "        # initialize weights of target networks\n",
    "        # 把net 赋值给target_network\n",
    "        self.target_q_net1.set_weights(\n",
    "            self.q_net1.get_weights())  # = self.target_ini(self.q_net1, self.target_q_net1)\n",
    "        self.target_q_net2.set_weights(\n",
    "            self.q_net2.get_weights())  # = self.target_ini(self.q_net2, self.target_q_net2)\n",
    "        self.target_policy_net.set_weights(\n",
    "            self.policy_net.get_weights())  # = self.target_ini(self.policy_net, self.target_policy_net)\n",
    "\n",
    "        self.update_cnt = 0  # 更新次数\n",
    "        self.policy_target_update_interval = policy_target_update_interval  # 策略网络更新频率\n",
    "\n",
    "        self.q_optimizer1 = tf.optimizers.Adam(q_lr)\n",
    "        self.q_optimizer2 = tf.optimizers.Adam(q_lr)\n",
    "        self.policy_optimizer = tf.optimizers.Adam(policy_lr)\n",
    "\n",
    "    # 在网络初始化的时候进行硬更新\n",
    "    def target_ini(self, net, target_net):\n",
    "        ''' hard-copy update for initializing target networks '''\n",
    "        for target_param, param in zip(target_net.trainable_weights, net.trainable_weights):\n",
    "            target_param.assign(param)\n",
    "        return target_net\n",
    "\n",
    "    # 在更新的时候进行软更新\n",
    "    def target_soft_update(self, net, target_net, soft_tau):\n",
    "        ''' soft update the target net with Polyak averaging '''\n",
    "        for target_param, param in zip(target_net.trainable_weights, net.trainable_weights):\n",
    "            target_param.assign(  # copy weight value into target parameters\n",
    "                target_param * (1.0 - soft_tau) + param * soft_tau\n",
    "                # 原来参数占比 + 目前参数占比\n",
    "            )\n",
    "        return target_net\n",
    "\n",
    "    def update(self, batch_size, eval_noise_scale, reward_scale=10., gamma=0.9, soft_tau=1e-3):\n",
    "        ''' update all networks in TD3 '''\n",
    "        self.update_cnt += 1  # 计算更新次数\n",
    "        state, action, reward, next_state = self.replay_buffer.sample(batch_size)  # 从buffer sample数据\n",
    "\n",
    "        reward = reward[:, np.newaxis]  # expand dim， 调整形状，方便输入网络\n",
    "\n",
    "        # 输入s',从target_policy_net计算a'。注意这里有加noisy的\n",
    "        ##改：此处为不加噪音，后续可与加入噪音进行对比\n",
    "        new_next_action = self.target_policy_net.predict(next_state)  # clipped normal noise#到底裁不裁？？？？？？？？？？？？？？？？？？？\n",
    "\n",
    "        # 归一化reward.(有正有负)\n",
    "        # reward = reward_scale * (reward - np.mean(reward, axis=0)) / (np.std(reward, axis=0) + 1e-6)  # normalize with batch mean and std; plus a small number to prevent numerical problem\n",
    "\n",
    "        # Training Q Function\n",
    "        # 把s'和a'堆叠在一起，一起输入到target_q_net。\n",
    "        # 有两个qnet，我们取最小值\n",
    "\n",
    "        target_q_input = tf.concat([next_state, new_next_action], 1)  # the dim 0 is number of samples\n",
    "        target_q_min = tf.minimum(self.target_q_net1.predict(target_q_input),\n",
    "                                  self.target_q_net2.predict(target_q_input))\n",
    "\n",
    "        # 计算target_q的值，用于更新q_net\n",
    "        # 之前有把done从布尔变量改为int，就是为了这里能够直接计算。\n",
    "        target_q_value = reward + gamma * target_q_min  # if done==1, only reward\n",
    "        state = state.astype('float32')\n",
    "        action = action.astype('float32')\n",
    "        action = action.reshape(64, 1)\n",
    "        q_input = tf.concat([state, action], 1)  # input of q_net\n",
    "\n",
    "        q_input = tf.dtypes.cast(q_input, tf.float32)\n",
    "        # 更新q_net1\n",
    "        # 这里其实和DQN是一样的\n",
    "        with tf.GradientTape() as q1_tape:\n",
    "            predicted_q_value1 = self.q_net1(q_input)\n",
    "            q_value_loss1 = tf.reduce_mean(tf.square(predicted_q_value1 - target_q_value))\n",
    "        q1_grad = q1_tape.gradient(q_value_loss1, self.q_net1.trainable_weights)\n",
    "        self.q_optimizer1.apply_gradients(zip(q1_grad, self.q_net1.trainable_weights))\n",
    "\n",
    "        # 更新q_net2\n",
    "        with tf.GradientTape() as q2_tape:\n",
    "            predicted_q_value2 = self.q_net2(q_input)\n",
    "            q_value_loss2 = tf.reduce_mean(tf.square(predicted_q_value2 - target_q_value))\n",
    "        q2_grad = q2_tape.gradient(q_value_loss2, self.q_net2.trainable_weights)\n",
    "        self.q_optimizer2.apply_gradients(zip(q2_grad, self.q_net2.trainable_weights))\n",
    "\n",
    "        # Training Policy Function\n",
    "        # policy不是经常update的，而qnet更新一定次数，才update一次\n",
    "        policy_loss_rec = False\n",
    "        if self.update_cnt % self.policy_target_update_interval == 0:\n",
    "            # 更新policy_net\n",
    "            with tf.GradientTape() as p_tape:\n",
    "                # 计算 action = Policy(s)，注意这里是没有noise的\n",
    "                new_action = self.policy_net(\n",
    "                    state\n",
    "                )  # no noise, deterministic policy gradients\n",
    "\n",
    "                # 叠加state和action\n",
    "                new_q_input = tf.concat([state, new_action], 1)\n",
    "                # ''' implementation 1 '''\n",
    "                # predicted_new_q_value = tf.minimum(self.q_net1(new_q_input),self.q_net2(new_q_input))\n",
    "                ''' implementation 2 '''\n",
    "                predicted_new_q_value = self.q_net1(new_q_input)\n",
    "                policy_loss = -tf.reduce_mean(predicted_new_q_value)  # 梯度上升\n",
    "                policy_loss_rec = policy_loss\n",
    "            p_grad = p_tape.gradient(policy_loss, self.policy_net.trainable_weights)\n",
    "            self.policy_optimizer.apply_gradients(zip(p_grad, self.policy_net.trainable_weights))\n",
    "            # print(self.policy_net.)\n",
    "\n",
    "            # Soft update the target nets\n",
    "            # 软更新target_network三个\n",
    "            self.target_q_net1 = self.target_soft_update(self.q_net1, self.target_q_net1, soft_tau)\n",
    "            self.target_q_net2 = self.target_soft_update(self.q_net2, self.target_q_net2, soft_tau)\n",
    "            self.target_policy_net = self.target_soft_update(self.policy_net, self.target_policy_net, soft_tau)\n",
    "\n",
    "        if policy_loss_rec:\n",
    "            return q_value_loss1, q_value_loss2, policy_loss_rec\n",
    "        else:\n",
    "            return q_value_loss1, q_value_loss2, 10086\n",
    "\n",
    "    def save_weights(self, model_path, describe=''):  # save trained weights\n",
    "        save_path = os.path.join(model_path, describe)\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        self.q_net1.save(save_path + '/model_q_net1.hdf5')\n",
    "        self.q_net2.save(save_path + '/model_q_net2.hdf5')\n",
    "        self.target_q_net1.save(save_path + '/model_target_q_net1.hdf5')\n",
    "        self.target_q_net2.save(save_path + '/model_target_q_net2.hdf5')\n",
    "        self.policy_net.save(save_path + '/model_policy_net.hdf5')\n",
    "        self.target_policy_net.save(save_path + '/model_target_policy_net.hdf5')\n",
    "        return save_path\n",
    "\n",
    "    # tl.files.save_npz(self.q_net1.trainable_weights, name='model_q_net1.npz')\n",
    "    # tl.files.save_npz(self.q_net2.trainable_weights, name='model_q_net2.npz')\n",
    "    # tl.files.save_npz(self.target_q_net1.trainable_weights, name='model_target_q_net1.npz')\n",
    "    # tl.files.save_npz(self.target_q_net2.trainable_weights, name='model_target_q_net2.npz')\n",
    "    # tl.files.save_npz(self.policy_net.trainable_weights, name='model_policy_net.npz')\n",
    "    # tl.files.save_npz(self.target_policy_net.trainable_weights, name='model_target_policy_net.npz')\n",
    "\n",
    "    def load_weights(self, save_path):  # load trained weights\n",
    "        self.q_net1.load_weights(save_path + '/model_q_net1.hdf5')\n",
    "        self.q_net2.load_weights(save_path + '/model_q_net2.hdf5')\n",
    "        self.target_q_net1.load_weights(save_path + '/model_target_q_net1.hdf5')\n",
    "        self.target_q_net2.load_weights(save_path + '/model_target_q_net2.hdf5')\n",
    "        self.policy_net.load_weights(save_path + '/model_policy_net.hdf5')\n",
    "        self.target_policy_net.load_weights(save_path + '/model_target_policy_net.hdf5')\n",
    "\n",
    "\n",
    "# tl.files.load_and_assign_npz(name='model_q_net1.npz', network=self.q_net1)\n",
    "# tl.files.load_and_assign_npz(name='model_q_net2.npz', network=self.q_net2)\n",
    "# tl.files.load_and_assign_npz(name='model_target_q_net1.npz', network=self.target_q_net1)\n",
    "# tl.files.load_and_assign_npz(name='model_target_q_net2.npz', network=self.target_q_net2)\n",
    "# tl.files.load_and_assign_npz(name='model_policy_net.npz', network=self.policy_net)\n",
    "# tl.files.load_and_assign_npz(name='model_target_policy_net.npz', network=self.target_policy_net)\n",
    "\n",
    "# 初始化环境\n",
    "model_name = get_str_time()\n",
    "env = SGTR_env(model=model, set_point=-56, train_datasets=train_datasets, mean=mean, std=std)  # 环境\n",
    "action_dim = env.action_space.shape[0]  # 动作空间\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "# 初始化缓冲区\n",
    "replay_buffer = ReplayBuffer(replay_buffer_size)\n",
    "\n",
    "# 初始化agent\n",
    "td3_trainer = TD3_Trainer(replay_buffer, hidden_dim=hidden_dim,\n",
    "                          policy_target_update_interval=policy_target_update_interval, action_range=action_range,\n",
    "                          q_lr=q_lr, policy_lr=policy_lr)\n",
    "rewards = []\n",
    "\n",
    "\n",
    "def fill_replay_buff():\n",
    "    global replay_buffer\n",
    "    with open(\n",
    "            r'M:\\work\\project_program\\bishe\\pyproject\\RL_pid_control_critic_actor\\强化学习+Lstm\\replay_buff_\\numpy_binary.npy',\n",
    "            'rb') as f:\n",
    "        buff_data = np.load(f, allow_pickle=True)\n",
    "    buff_datas = buff_data.tolist()\n",
    "    print('Filling Replay Buffer.......')\n",
    "    for data in tqdm(buff_datas):\n",
    "        state, action, reward, next_state = data\n",
    "        replay_buffer.push(state, action, reward, next_state)\n",
    "\n",
    "\n",
    "def train(new: bool, thread_name: str):\n",
    "    global model_name, env, action_dim, state_dim, replay_buffer, td3_trainer, rewards\n",
    "    print(\"initializing.......\")\n",
    "    if not new:\n",
    "        fill_replay_buff()\n",
    "        # 接着上一次训练\n",
    "        td3_trainer.load_weights(\n",
    "            r'M:\\work\\project_program\\bishe\\pyproject\\RL_pid_control_critic_actor\\强化学习+Lstm\\rl_model\\last_model\\_2022_7_23_21_3_4_96_0')\n",
    "        # 获得上一次的奖励\n",
    "        reward_data = pd.read_csv(\n",
    "            r\"M:\\work\\project_program\\bishe\\pyproject\\RL_pid_control_critic_actor\\强化学习+Lstm\\rewards_recoard\\_2022_7_23_21_3_4_rewards.csv\")\n",
    "        reward = reward_data[\"rewards\"].values.tolist()\n",
    "    else:\n",
    "        reward = []\n",
    "\n",
    "    frame_idx = 0  # 总步数\n",
    "    rewards = reward  # 记录每个EP的总reward\n",
    "    max_reward = 0  # 记录最大reward\n",
    "    # time_spend = []\n",
    "    # t0 = time.time ()\n",
    "    # t_start = time.time ()\n",
    "    try:\n",
    "        while frame_idx < max_frames:\n",
    "            # 小于最大步数，就继续训练\n",
    "            # time_temp = {\"frame_idx\": frame_idx}\n",
    "\n",
    "            state = env.reset()[-1]  # 初始化state\n",
    "            state = state.astype(np.float32)  # 整理state的类型\n",
    "            episode_reward = 0\n",
    "            # time_temp[\"初始化state\"] = time.time () - t0\n",
    "\n",
    "            step_count = 0\n",
    "            # 开始训练\n",
    "\n",
    "            for step in range(max_steps):\n",
    "                t0 = time.time()\n",
    "                if frame_idx > explore_frams:  # 如果小于500步，就随机，如果大于就用get-action\n",
    "                    action = td3_trainer.policy_net.predict(\n",
    "                        np.array([state])) + tf.clip_by_value(tf.random.normal([1], 0, 0.006), -1, 1)\n",
    "                    action = action.numpy().tolist()[0][0]\n",
    "                else:\n",
    "                    #指数分布随机\n",
    "                    # action = tf.clip_by_value (random.expovariate (1 / 0.878413) - 0.878413,action_min,\n",
    "                    #                            action_max)  # tf.random.uniform([1], action_min, action_max)+tf.clip_by_value(tf.random.normal([1],0,0.006),-1,1)\n",
    "                    # action=action.numpy()\n",
    "                    # 平均分布随机\n",
    "                    aaa = tf.random.uniform([1], -1, 1)\n",
    "                    action = (action_max - action_min) * aaa.numpy().tolist()[0]\n",
    "                #time_temp[\"选择action\"] = time.time () - t0\n",
    "\n",
    "                # 与环境进行交互\n",
    "                # t0 = time.time ()\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                if frame_idx > explore_frams:\n",
    "                    reward += random.random() / 10\n",
    "                next_state = next_state.astype(np.float32)\n",
    "                action = np.array(action).tolist()\n",
    "                done = 1 if done == True else 0\n",
    "                #time_temp[\"与环境交互\"] = time.time () - t0\n",
    "\n",
    "                # 记录数据在replay_buffer\n",
    "                # t0 = time.time ()\n",
    "                replay_buffer.push(state, action, reward, next_state)\n",
    "\n",
    "                # 赋值state，累计总reward，步数\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                # time_temp[\"记录数据\"] = time.time () - t0\n",
    "\n",
    "                print(\n",
    "                    f'Thread:{thread_name} | Episode: {frame_idx}/{max_frames}  | Action: {action} | Step: {step} | Step Reward: {round(reward, 3)} |',\n",
    "                    end='\\r')\n",
    "\n",
    "                if done:\n",
    "                    step_count = step\n",
    "                    break\n",
    "\n",
    "                # 如果数据超过一个batch_size的大小，那么就开始更新\n",
    "                t0 = time.time()\n",
    "                if len(replay_buffer) >= 29999 and step_count > 100:\n",
    "                    for i in range(1):  # 注意：这里更新可以更新多次！\n",
    "                        td3_trainer.update(batch_size, eval_noise_scale=0.1, reward_scale=0.1)\n",
    "                # time_temp[\"td3网络更新\"] = time.time () - t0\n",
    "                # time_spend.append (time_temp)\n",
    "                step_count = step\n",
    "\n",
    "            print(\n",
    "                'Thread:{} | Episode: {}/{}  | Episode Reward: {:.4f}  | Run Step: {} |--------Process Failed Flag-------'.format(\n",
    "                    thread_name, frame_idx, max_frames, episode_reward, step_count))\n",
    "\n",
    "            frame_idx += 1\n",
    "\n",
    "            # t0 = time.time ()\n",
    "            if step_count > 100 and episode_reward > max_reward:  # 保存奖励最大的模型\n",
    "                max_reward = episode_reward\n",
    "                td3_trainer.save_weights('rl_model/best_model', model_name + str(frame_idx) + \"_\" + str(max_reward))\n",
    "            # print (f\"模型保存：{time.time () - t0}\\n#################\")\n",
    "            # print(episode_reward,'###',state)\n",
    "            rewards.append(episode_reward)\n",
    "\n",
    "    except:\n",
    "        rewards_rec = pd.DataFrame(data={'rewards': rewards})\n",
    "        rewards_rec.to_csv(r\"rewards_recoard/\" + model_name + 'rewards.csv')\n",
    "        td3_trainer.save_weights('rl_model/last_model', model_name + str(frame_idx) + \"_\" + str(max_reward))\n",
    "        replay_data = np.array(replay_buffer.buffer, dtype='object')\n",
    "        np.save(\"replay_buff_/numpy_binary\", replay_data)\n",
    "\n",
    "    rewards_rec = pd.DataFrame(data={'rewards': rewards})\n",
    "    rewards_rec.to_csv(r\"rewards_recoard/\" + model_name + 'rewards.csv')\n",
    "    td3_trainer.save_weights('rl_model/last_model', model_name + str(frame_idx) + \"_\" + str(max_reward))\n",
    "    replay_data = np.array(replay_buffer.buffer, dtype='object')\n",
    "    np.save(\"replay_buff_/numpy_binary\", replay_data)\n",
    "\n",
    "\n",
    "def test():\n",
    "    global model_name, env, action_dim, state_dim, replay_buffer, td3_trainer\n",
    "    # 装载模型权重\n",
    "    td3_trainer.load_weights(\n",
    "        r'M:\\work\\project_program\\bishe\\pyproject\\RL_pid_control_critic_actor\\强化学习+Lstm\\rl_model\\last_model\\_2022_7_23_11_28_26_794_76.59812447160093')\n",
    "    for test_count in range(10):\n",
    "        state = env.reset()[-1]  # 初始化state\n",
    "        state = state.astype(np.float32)  # 整理state的类型\n",
    "        speed_rec = []\n",
    "        temp_rec = []\n",
    "        action_rec = []\n",
    "        reward_sum = []\n",
    "        episode_reward = 0\n",
    "        for step in tqdm(range(max_steps)):\n",
    "            action = td3_trainer.policy_net.predict(\n",
    "                np.array([state]))  # + tf.clip_by_value (tf.random.normal ([1],0,0.006),-1,1)\n",
    "            action = action.data.tolist()[0][0]\n",
    "            action_rec.append(action)\n",
    "            # 与环境进行交互\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = next_state.astype(np.float32)\n",
    "            action = np.array(action).tolist()\n",
    "            done = 1 if done == True else 0\n",
    "            print(reward)\n",
    "            episode_reward += reward\n",
    "            reward_sum.append(episode_reward)\n",
    "\n",
    "            ori_temp_before_last = env.cal_origin_val(1, state[1])\n",
    "            ori_temp_last = env.cal_origin_val(1, next_state[1])\n",
    "            temp_rec.append(ori_temp_before_last)\n",
    "            # 赋值state，累计总reward，步数\n",
    "            state = next_state\n",
    "\n",
    "            temp_change_speed = (ori_temp_last - ori_temp_before_last) * 3600\n",
    "            speed_rec.append(temp_change_speed)\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(speed_rec)\n",
    "        plt.title('speed_rec')\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(temp_rec)\n",
    "        plt.title('temp_rec')\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(action_rec)\n",
    "        plt.title('action_rec')\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(reward_sum)\n",
    "        plt.title('reward_sum')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_reward():\n",
    "    global env\n",
    "    temp = []\n",
    "    xlist = []\n",
    "    for i in range(int(10000 / 0.01)):\n",
    "        xlist.append(-5000 + 0.01 * i)\n",
    "\n",
    "    for i in xlist:\n",
    "        aaa = env.cal_reward(i)\n",
    "        temp.append(aaa)\n",
    "    plt.plot(xlist, temp)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # def multi_thread ():\n",
    "    # \tthreads = []\n",
    "    # \tfor i in range(10):\n",
    "    # \t\tthreads.append (\n",
    "    # \t\t\tthreading.Thread (target=train,args=(True,f\"Thread{i}\"))\n",
    "    # \t\t)\n",
    "    #\n",
    "    # \tfor thread in threads:\n",
    "    # \t\tthread.start ()\n",
    "    #\n",
    "    # \tfor thread in threads:\n",
    "    # \t\tthread.join ()\n",
    "    #\n",
    "    # try:\n",
    "    # \tmulti_thread()\n",
    "    # except:\n",
    "    # \trewards_rec = pd.DataFrame (data={'rewards': rewards})\n",
    "    # \trewards_rec.to_csv (r\"rewards_recoard/\" + model_name + 'rewards.csv')\n",
    "    # \ttd3_trainer.save_weights ('rl_model/last_model',model_name)\n",
    "    # \treplay_data = np.array (replay_buffer.buffer,dtype='object')\n",
    "    # \tnp.save (\"numpy_binary\",replay_data)\n",
    "\n",
    "    #train(True,\"Thread0\")\n",
    "\n",
    "    train(True, \"Thread0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}